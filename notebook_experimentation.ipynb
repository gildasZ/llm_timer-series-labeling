{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *__Check first before starting__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Do 'pipenv install ipykernel' if you get error.\n",
    "print(\"Kernel is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\gilda\\OneDrive\\Documents\\_NYCU\\MASTER_S_studies\\Master's Thesis\\LABORATORY\\_Global_Pytorch\\llm_timer-series-labeling-base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the working directory to the project root\n",
    "# Working_directory = os.path.normpath(\"C:/Users/gilda/OneDrive/Documents/_NYCU/MASTER_S_studies/Master\\'s Thesis/LABORATORY/_Global_Pytorch/Continual_Learning\")\n",
    "Working_directory = os.path.normpath(\"C:/Users/gilda/OneDrive/Documents/_NYCU/MASTER_S_studies/Master\\'s Thesis/LABORATORY/_Global_Pytorch/llm_timer-series-labeling-base\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")  # Prints the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **__All imports__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from typing import Callable, Tuple\n",
    "import contextlib\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __**All functions (For data processing)**__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for random, numpy, and torch for reproducible results.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # These two are important for reproducibility with CUDA\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Global seed set to {seed}\")\n",
    "\n",
    "def plot_with_matplotlib(data: pd.DataFrame, \n",
    "                         title: str, \n",
    "                         interactive: bool = False, \n",
    "                         save_path: str = None, \n",
    "                         show_plot: bool = True, \n",
    "                         save_matplotlib_object: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot data using Matplotlib, with optional interactivity using mpl-interactions.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The data to plot, must contain 'close' column.\n",
    "    - title (str): The title of the plot.\n",
    "    - interactive (bool): If True, enables interactive zoom and pan.\n",
    "    - save_path (Optional[str]): If provided, saves the plot to this path.\n",
    "    - show_plot (bool): If True, displays the plot. If False, skips display.\n",
    "    - save_matplotlib_object (Optional[str]): If provided, saves the Matplotlib figure object to this file path.\n",
    "    \"\"\"\n",
    "    if not all(col in data.columns for col in ['close']):\n",
    "        raise ValueError(\"The input DataFrame must contain 'close' column.\")\n",
    "\n",
    "    # Use the default Matplotlib color cycle for the line\n",
    "    default_blue = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "    print(f\"Default blue: {default_blue}\")\n",
    "\n",
    "    # Explicit colors for trends\n",
    "    trend_colors = {\n",
    "        0: 'black',\n",
    "        1: 'yellow',\n",
    "        2: 'red',\n",
    "        3: 'green',\n",
    "        4: default_blue #'purple',\n",
    "    }\n",
    "    # unique_trends = [0, -25, -15, 15, 25]\n",
    "    # colormap = plt.cm.get_cmap('tab10', len(unique_trends))  # Choose 'tab10' or 'Set1' for distinct colors\n",
    "    # trend_colors = {trend: colormap(i) for i, trend in enumerate(unique_trends)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot data as a single connected line, colored by trend\n",
    "    if 'trend' in data.columns:\n",
    "        legend_added = set() # Track which trends have already been added to the legend\n",
    "        prev_idx = data.index[0]\n",
    "        for idx, row in data.iterrows():\n",
    "            if idx != prev_idx:\n",
    "                trend_key = int(row['trend'])  # Convert trend value to int for lookup\n",
    "                label = f'Trend {trend_key}' if trend_key not in legend_added else None\n",
    "                ax.plot([prev_idx, idx], \n",
    "                        [data.loc[prev_idx, 'close'], row['close']],\n",
    "                        color=trend_colors[trend_key], \n",
    "                        linestyle='-', \n",
    "                        # marker='o', \n",
    "                        linewidth=1,\n",
    "                        label=label  # Add label only if it's not in the legend\n",
    "                )\n",
    "                legend_added.add(trend_key)  # Mark this trend as added to the legend\n",
    "            prev_idx = idx\n",
    "\n",
    "        ax.set_title(f\"{title} (Connected, Colored by Trend)\")\n",
    "    else:\n",
    "        # Default plot if no trend column exists\n",
    "        ax.plot(data.index, data['close'], label='Closing Price', linestyle='-', marker='o', \n",
    "                markersize=2, linewidth=1, color=default_blue, markerfacecolor='green', markeredgecolor='black')\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing Price (USD)')\n",
    "    \n",
    "    # Add a legend manually for trends\n",
    "    # for trend, color in trend_colors.items():\n",
    "    #     ax.plot([], [], color=color, label=f'Trend {trend}')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    # Enable interactivity if requested\n",
    "    if interactive:\n",
    "        zoom_factory(ax)  # Enable zoom with mouse wheel\n",
    "        panhandler(fig)   # Enable panning with left-click\n",
    "        print(\"Interactive mode enabled. Use mouse wheel to zoom and left click to pan.\")\n",
    "\n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        fig.tight_layout()  # Ensures the layout is clean\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "    # Save the Matplotlib figure object\n",
    "    if save_matplotlib_object:\n",
    "        with open(save_matplotlib_object, 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "        print(f\"Matplotlib figure object saved to: {save_matplotlib_object}\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plot display skipped.\")\n",
    "\n",
    "def load_and_show_pickle(pickle_file_path: str):\n",
    "    \"\"\"\n",
    "    Load a pickled Matplotlib figure object and display it with optional interactivity.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_file_path (str): Path to the pickled Matplotlib figure file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the pickle file and load the figure\n",
    "        with open(pickle_file_path, \"rb\") as f:\n",
    "            loaded_fig = pickle.load(f)\n",
    "\n",
    "        print(f\"Figure successfully loaded and displayed from: {pickle_file_path}\")\n",
    "\n",
    "        # Use plt.show() to allow interactivity\n",
    "        plt.show(block=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pickle_file_path}. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the pickled figure: {e}\")\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data to be saved.\n",
    "        file_path (str): The file path (including the file name) to save the CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path)\n",
    "    # df_to_save = df.copy()\n",
    "    # df_to_save[\"date\"] = df_to_save.index.strftime('%Y-%m-%d %H:%M:%S')  # Add formatted index as a column\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    # df_to_save.to_csv(file_path)\n",
    "    print(f\"\\nSuccessfully saved data with moving average to CSV: \\n\\t{file_path}\\n\")\n",
    "\n",
    "def read_csv_file(file_path: str, preview_rows: int = 5, \n",
    "                  days_towards_end: int = None, \n",
    "                  days_from_start: int = None, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a pandas DataFrame filtered by date range.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        preview_rows (int): Number of rows to preview (default is 5).\n",
    "        days_towards_end (int, optional): Number of days from the most recent date to retrieve data.\n",
    "        days_from_start (int, optional): Number of days from the oldest date of the filtered data to retrieve data.\n",
    "        description (str): A brief description of the dataset being loaded.\n",
    "                           Explanation:\n",
    "                           - To retrieve data from the **end**: Use `days_towards_end`.\n",
    "                           - To retrieve data from the **start of the filtered range**: Use `days_from_start`.\n",
    "                           - To retrieve data from the **middle**: Use both:\n",
    "                             For example, if `days_towards_end=100` and `days_from_start=50`,\n",
    "                             the function will first filter the last 100 days of the dataset,\n",
    "                             and then filter the first 50 days from this range.\n",
    "                             This results in data between the last 100th and the last 50th day.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The loaded and filtered data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if description:\n",
    "            print(f\"\\nDescription: {description}\")\n",
    "        print(f\"\\nFile path: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
    "        \n",
    "        # Filter by days towards the end\n",
    "        if days_towards_end is not None:\n",
    "            last_date = data.index.max()  # Get the most recent date in the dataset\n",
    "            end_cutoff_date = last_date - pd.Timedelta(days=days_towards_end)\n",
    "            data = data[data.index >= end_cutoff_date]\n",
    "            print(f\"\\nRetrieving data from the past {days_towards_end} days (from {end_cutoff_date.date()} onwards):\")\n",
    "        \n",
    "        # Filter by days from the start (from the filtered data)\n",
    "        if days_from_start is not None:\n",
    "            first_date = data.index.min()  # Get the earliest date in the filtered dataset\n",
    "            start_cutoff_date = first_date + pd.Timedelta(days=days_from_start)\n",
    "            data = data[data.index <= start_cutoff_date]\n",
    "            print(f\"\\nRetrieving the first {days_from_start} days from the filtered data (up to {start_cutoff_date.date()}):\")\n",
    "\n",
    "        if preview_rows:\n",
    "            # Print a preview of the data\n",
    "            print(f\"\\nPreview of the first {preview_rows} rows:\")\n",
    "            # print(data.head(preview_rows), '\\n')\n",
    "            display(data.head(preview_rows))\n",
    "            print()\n",
    "\n",
    "            print(f\"\\nPreview of the last {preview_rows} rows:\")\n",
    "            # print(data.tail(preview_rows), '\\n')\n",
    "            display(data.tail(preview_rows))\n",
    "            print()\n",
    "\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The file could not be parsed. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def downsample_minute_data(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsample minute data into N-minute intervals by retaining every Nth row.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The original DataFrame with a datetime index.\n",
    "        n (int): The number of minutes for the downsampling interval.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Downsampled DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n========---> Downsampling the data! \\n\")\n",
    "    data = data.copy()\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            data.index = pd.to_datetime(data.index)  # Convert to DatetimeIndex\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"The DataFrame index could not be converted to DatetimeIndex.\") from e\n",
    "\n",
    "    # Filter rows where the minute index modulo N is 0\n",
    "    downsampled_data = data[data.index.minute % n == 0]\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "def calculate_log_returns_all_columns(data: pd.DataFrame, exclude_columns: list = [], dropna: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate log returns for all numeric columns in a pandas DataFrame,\n",
    "    excluding specified columns, and removing excluded columns from the returned DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing numeric data.\n",
    "        exclude_columns (list): List of columns to exclude from log return calculations and the result.\n",
    "        dropna (bool): Whether to drop rows with NaN values resulting from the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with log returns for numeric columns,\n",
    "                      excluding specified columns.\n",
    "    \"\"\"\n",
    "    data = data.copy().drop(columns=exclude_columns)\n",
    "    columns_to_transform = data.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"columns_to_transform = \\n{columns_to_transform}, \\nlen(columns_to_transform) = {len(columns_to_transform)}\")\n",
    "\n",
    "    for col in columns_to_transform:\n",
    "        # Ensure no negative or zero values\n",
    "        if (data[col] <= 0).any():\n",
    "            raise ValueError(f\"Column '{col}' contains non-positive values. Log returns require strictly positive values.\")\n",
    "        data[col] = np.log(data[col] / data[col].shift(1))\n",
    "\n",
    "    # Optionally drop rows with NaN values\n",
    "    return data.dropna() if dropna else data\n",
    "\n",
    "def created_sequences_2(data: pd.DataFrame, sequence_length: int = 60, sliding_interval: int = 60) -> list:\n",
    "    \"\"\"\n",
    "    Divide the dataset into sequences based on the sequence_length.\n",
    "    Each sequence must fully cover the window size.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): The input DataFrame.\n",
    "    - sequence_length (int): The window size for sequences.\n",
    "\n",
    "    Returns:\n",
    "    - sequences (list): A list of sequences (as DataFrames).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, sliding_interval):\n",
    "        # print(f\"Processing sequence starting at index: {i}\")\n",
    "        seq = data.iloc[i:i + sequence_length].copy()\n",
    "        sequences.append(seq)\n",
    "    # print(f\"Total sequences created: {len(sequences)}\")\n",
    "    return sequences\n",
    "\n",
    "def gaussian_smoothing(data: pd.DataFrame, sigma=2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to numeric columns in a DataFrame and ensures the index is sorted in ascending order.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        sigma (float): Standard deviation for the Gaussian kernel. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with smoothed numeric columns and sorted index.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by index in ascending order\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):  # Only apply to numeric columns\n",
    "            data[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    return data\n",
    "\n",
    "def detect_trends_4(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.001, \n",
    "    upper_threshold: float = 0.02,\n",
    "    reverse_steps: int = 7,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects trends based on log return data provided in a specified column and categorizes them into different strength levels.\n",
    "\n",
    "    This function analyzes time-series data by evaluating cumulative trends in log return values provided in the input DataFrame. \n",
    "    It uses three dictionaries (`dic1`, `dic2`, `dic3`) to track different phases of trends, handles multi-step reversals, and \n",
    "    classifies trends dynamically based on cumulative product thresholds and specified thresholds for trend strengths.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing log return data.\n",
    "        column (str): Column name containing log return values. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.001.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.02.\n",
    "        reverse_steps (int): Number of consecutive steps to confirm a trend reversal. Defaults to 7.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added column:\n",
    "                    - 'trend': Categorized trend values based on the detected phases:\n",
    "                        - 0: No trend\n",
    "                        - 1: Moderate negative trend\n",
    "                        - 2: Very strong negative trend\n",
    "                        - 3: Moderate positive trend\n",
    "                        - 4: Very strong positive trend\n",
    "                      Any trends not included in `trends_to_keep` will be reset to 0.\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Assumption**:\n",
    "    - The input DataFrame already contains log return data in the specified column (`column`).\n",
    "\n",
    "    2. **Trend Tracking**:\n",
    "    - Uses dictionaries to monitor trends:\n",
    "        - `dic1`: Tracks the first phase of the trend.\n",
    "        - `dic2`: Tracks the second phase if a reversal occurs.\n",
    "        - `dic3`: Tracks the third phase if another reversal occurs.\n",
    "\n",
    "    3. **Cumulative Product**:\n",
    "    - Calculates the cumulative product of `(1 + log_return)` from the specified column to evaluate the strength of trends.\n",
    "\n",
    "    4. **Reversal Handling**:\n",
    "    - If a trend reversal persists beyond `reverse_steps`, labels are assigned based on the cumulative product tracked in `dic1`.\n",
    "    - Subsequent reversals are merged or labeled independently if conditions are met.\n",
    "\n",
    "    5. **Label Assignment**:\n",
    "    - Labels are dynamically assigned based on cumulative product thresholds for positive and negative trends:\n",
    "        - Positive trends are categorized as moderate (3) or strong (4).\n",
    "        - Negative trends are categorized as moderate (1) or strong (2).\n",
    "\n",
    "    6. **Trend Filtering**:\n",
    "    - After detecting trends, only those specified in `trends_to_keep` remain unchanged.\n",
    "    - Any trend category not included in `trends_to_keep` is reset to 0 (No Trend).\n",
    "\n",
    "    7. **Edge Cases**:\n",
    "    - Properly handles scenarios where data points are insufficient for trend analysis or when trend phases overlap, ensuring all data points are labeled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    df['trend'] = None  # Default value \n",
    "    \n",
    "    # print(\"\\n#-------------------- Working on 'trend' patterns -----------------------#\")\n",
    "    dic1, dic2, dic3 = None, None, None # Initialize trend tracking dictionaries\n",
    "    # dic1 = None # {'ids': [], 'last_sign': None, 'cumulative': 1.0}\n",
    "    \n",
    "    def assign_label(dictio_, lower_threshold, upper_threshold):\n",
    "        cumulative = dictio_['cumulative']\n",
    "        # print(f\"cumulative = {cumulative}\")\n",
    "        if cumulative > (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 4  # Very strong positive\n",
    "        elif (1 + lower_threshold) < cumulative <= (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 3  # Moderate positive\n",
    "        elif (1 - upper_threshold) < cumulative <= (1 - lower_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 1  # Moderate negative\n",
    "        elif cumulative <= (1 - upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 2  # Very strong negative\n",
    "        else:\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 0  # No trend\n",
    "    \n",
    "    #----------------------- For Loop -----------------------#\n",
    "    for idx, log_ret in enumerate(df[column]):\n",
    "        sign = 1 if log_ret > 0 else -1\n",
    "\n",
    "        if dic1 is None:  # Initialize dic1\n",
    "            # print(f\"\\nThis one time condition 'if loop' is running \\n\")\n",
    "            dic1 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic1['last_sign']\n",
    "        if sign == last_sign and dic2 is None:  # Continue same trend\n",
    "            dic1['ids'].append(idx)\n",
    "            dic1['last_sign'] = sign\n",
    "            dic1['cumulative'] *= (1 + log_ret)\n",
    "            continue\n",
    "\n",
    "        # 1st Reversal occuring\n",
    "        if dic2 is None:  # Start dic2\n",
    "            dic2 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic2['last_sign']\n",
    "        if sign == last_sign and dic3 is None:  # Continue same trend\n",
    "            dic2['ids'].append(idx)\n",
    "            dic2['last_sign'] = sign\n",
    "            dic2['cumulative'] *= (1 + log_ret)\n",
    "            if len(dic2['ids']) == reverse_steps:\n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                # print(f\"dic1['cumulative'] = {dic1['cumulative']}, and dic1['ids'] = {dic1['ids']}\")\n",
    "                dic1 = dic2\n",
    "                dic2 = None\n",
    "                # print(f\"dic1 after trend reversal persisted and dic1 = dic2 = \\n{dic1}\")\n",
    "                # print(f\"dic2 after being reset: {dic2}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 2nd Reversal occuring\n",
    "        if dic3 is None:  # Start dic3\n",
    "            dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic3['last_sign']\n",
    "        if sign == last_sign: # Continue same trend, there is no dic4 to check if is None\n",
    "            dic3['ids'].append(idx)\n",
    "            dic3['last_sign'] = sign\n",
    "            dic3['cumulative'] *= (1 + log_ret)\n",
    "            dic_prod = dic2['cumulative'] * dic3['cumulative']\n",
    "            # if (sign == 1 and dic1['cumulative'] * dic_prod > dic1['cumulative']) or (sign == -1 and dic1['cumulative'] * dic_prod < dic1['cumulative'])):\n",
    "            if (sign == 1 and dic_prod > 1) or (sign == -1 and dic_prod < 1): # More beautiful\n",
    "                # Merge dic1, dic2, and dic3\n",
    "                dic1['ids'] += dic2['ids'] + dic3['ids']\n",
    "                dic1['last_sign'] = dic3['last_sign']\n",
    "                dic1['cumulative'] *= dic2['cumulative'] * dic3['cumulative']\n",
    "                dic2, dic3 = None, None\n",
    "                continue\n",
    "\n",
    "            if len(dic3['ids']) == reverse_steps:      \n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                assign_label(dic2, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1 = dic3\n",
    "                dic2, dic3 = None, None\n",
    "                # print(f\"dic2 after 2nd trend reversal didn't catch up fast enough, and now \\ndic1 = dic3 = {dic1}\")\n",
    "                # print(f\"dic3 and dic2 after being reset: {dic3}\\n\")\n",
    "            continue\n",
    "            \n",
    "        # 3rd Reversal occuring\n",
    "        assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "        # Reassign values\n",
    "        dic1 = dic2\n",
    "        dic2 = dic3\n",
    "        dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "        # print(f\"There was a 3rd trend reversal, and now \\ndic1 = dic2 = {dic1}, \\ndic2 = dic3 = {dic2}\")\n",
    "        # print(f\"dic3 after being reset: {dic3}\\n\")\n",
    "\n",
    "    # Assign remaining labels\n",
    "    if dic1:\n",
    "        assign_label(dic1, lower_threshold, upper_threshold)\n",
    "    if dic2:\n",
    "        assign_label(dic2, lower_threshold, upper_threshold)\n",
    "    if dic3:\n",
    "        assign_label(dic3, lower_threshold, upper_threshold)\n",
    "    # print(\"\\n#-------------------- Returning 'trend' patterns ------------------------#\")\n",
    "    \n",
    "    # Apply filtering: Keep only selected trends, set others to 0\n",
    "    df['trend'] = df['trend'].apply(lambda x: x if x in trends_to_keep else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_X_y(sequences: list[pd.DataFrame], \n",
    "              target_column: str = 'trend',\n",
    "              detect_trends_function: Callable[[pd.DataFrame, str, float, float, int, set], pd.DataFrame] = detect_trends_4, \n",
    "              column: str = 'close', \n",
    "              lower_threshold: float = 0.0009, \n",
    "              upper_threshold: float = 0.015,\n",
    "              reverse_steps: int = 7,\n",
    "              trends_to_keep: set = {0, 1, 2, 3, 4}) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process sequences to generate features (X) and labels (y) while applying trend detection.\n",
    "\n",
    "    Args:\n",
    "    - sequences (list of pd.DataFrame): List of DataFrame sequences.\n",
    "    - lower_threshold (float): Lower threshold for trend detection.\n",
    "    - upper_threshold (float): Upper threshold for trend detection.\n",
    "    - reverse_steps (int): Steps to reverse trends in the sequence.\n",
    "    - target_column (str): Column name to use as the label (default: 'trend').\n",
    "    - detect_trends_function (Callable): Function for detecting trends, defaults to `detect_trends_4`.\n",
    "    - trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend).\n",
    "\n",
    "    Returns:\n",
    "    - X (np.ndarray): Features array of shape (num_sequences, sequence_length, num_features).\n",
    "    - y (np.ndarray): Labels array of shape (num_sequences,).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # count = 0\n",
    "    for seq in sequences:\n",
    "        # Apply trend detection on the sequence\n",
    "        seq = detect_trends_function(seq, column=column, \n",
    "                                     lower_threshold=lower_threshold, \n",
    "                                     upper_threshold=upper_threshold, \n",
    "                                     reverse_steps=reverse_steps,\n",
    "                                     trends_to_keep=trends_to_keep)\n",
    "        # if count == 0:\n",
    "        #     count = 1\n",
    "        #     print(f\"\\nseq.head()\")\n",
    "        #     display(seq.head())\n",
    "        #     print()\n",
    "        # Extract features (X) and labels (y)\n",
    "        X.append(seq.drop(columns=[target_column]).values)  # All but the target column\n",
    "        y.append(seq[target_column].values)  # Target column as labels\n",
    "        \n",
    "    # return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def process_and_return_splits(\n",
    "    with_indicators_file_path: str,\n",
    "    downsampled_data_minutes: int,\n",
    "    exclude_columns: list[str],\n",
    "    lower_threshold: float,\n",
    "    upper_threshold: float,\n",
    "    reverse_steps: int,\n",
    "    sequence_length: int,\n",
    "    sliding_interval: int,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> tuple[\n",
    "    list[list[float]],  # X_train: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_train: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_val: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_val: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_test: List of sequences, each containing a list of features\n",
    "    list[list[int]]     # y_test: List of sequences, each containing a list of labels\n",
    "]:\n",
    "    \"\"\"\n",
    "    Processes time-series data from a CSV file and prepares it for machine learning.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Reads data from the specified CSV file and sorts it by date in descending order.\n",
    "        2. Optionally downsamples the data to a lower frequency (e.g., 5-minute intervals).\n",
    "        3. Applies Gaussian smoothing to reduce noise in the data.\n",
    "        4. Calculates log returns for all numeric columns, excluding specified columns.\n",
    "        5. Detects trends based on defined thresholds (`lower_threshold`, `upper_threshold`, and `reverse_steps`).\n",
    "        6. Filters trends to keep only those specified in `trends_to_keep`, setting others to 0 (No Trend).\n",
    "        7. Converts the processed data into sequences of a fixed length (`sequence_length`) with a sliding interval.\n",
    "        8. Splits the sequences into training (80%), validation (10%), and test (10%) sets.\n",
    "        9. Further splits the sequences into features (`X`) and labels (`y`) for supervised learning.\n",
    "\n",
    "    Args:\n",
    "        with_indicators_file_path (str): Path to the CSV file containing the time-series data.\n",
    "        downsampled_data_minutes (int): Frequency for downsampling the data (e.g., 1 for no downsampling).\n",
    "        exclude_columns (list[str]): List of column names to exclude from log return calculations.\n",
    "        lower_threshold (float): Lower threshold for trend detection.\n",
    "        upper_threshold (float): Upper threshold for trend detection.\n",
    "        reverse_steps (int): Number of steps for reversing trends in trend detection.\n",
    "        sequence_length (int): Length of sequences to create from the data.\n",
    "        sliding_interval (int): Interval for sliding the window when creating sequences.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[list[float]], list[list[int]], list[list[float]], list[list[int]], list[list[float]], list[list[int]]]:\n",
    "            A tuple containing:\n",
    "            - X_train (list[list[float]]): Input sequences for training.\n",
    "            - y_train (list[list[int]]): Target sequences for training.\n",
    "            - X_val (list[list[float]]): Input sequences for validation.\n",
    "            - y_val (list[list[int]]): Target sequences for validation.\n",
    "            - X_test (list[list[float]]): Input sequences for testing.\n",
    "            - y_test (list[list[int]]): Target sequences for testing.\n",
    "\n",
    "    Example:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = process_and_return_splits(\n",
    "            with_indicators_file_path=\"data.csv\",\n",
    "            downsampled_data_minutes=5,\n",
    "            exclude_columns=[\"volume\"],\n",
    "            lower_threshold=-0.05,\n",
    "            upper_threshold=0.05,\n",
    "            reverse_steps=3,\n",
    "            sequence_length=50,\n",
    "            sliding_interval=5,\n",
    "            trends_to_keep={1, 2, 3, 4}  # Only keep categorized trends, set others to 0\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    data_retrieved = read_csv_file(with_indicators_file_path, preview_rows=0) # 190 days of data\n",
    "    data_retrieved = data_retrieved.sort_index(ascending=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    if downsampled_data_minutes != 1:\n",
    "        print(\"Downsampling the data! \\n\")\n",
    "        data_retrieved = downsample_minute_data(data_retrieved, downsampled_data_minutes)\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_retrieved.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_retrieved.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_retrieved.index.tz,\n",
    "    ).difference(data_retrieved.index)\n",
    "    print(f\"\\ndata_retrieved - Missing timestamps time: \\n{missing_timestamps}\") \n",
    "\n",
    "    data_gaussian = gaussian_smoothing(data_retrieved, sigma=7)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_gaussian.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_gaussian.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_gaussian.index.tz,\n",
    "    ).difference(data_gaussian.index)\n",
    "    print(f\"\\ndata_gaussian - Missing timestamps time: \\n{missing_timestamps}\\n\")\n",
    "\n",
    "    data_log_return = calculate_log_returns_all_columns(data_gaussian, exclude_columns=exclude_columns)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_log_return.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_log_return.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_log_return.index.tz,\n",
    "    ).difference(data_log_return.index)\n",
    "    print(f\"\\ndata_log_return - Missing timestamps time: \\n{missing_timestamps}\\n\") \n",
    "\n",
    "    # Check if there are missing timestamps\n",
    "    if missing_timestamps.empty:\n",
    "        print(\"No missing timestamps.\")\n",
    "    else:\n",
    "        for timestamp in missing_timestamps:\n",
    "            print(f\"\\nMissing timestamp: {timestamp}\")\n",
    "            \n",
    "            # Create a placeholder for the missing timestamp\n",
    "            if timestamp not in data_log_return.index:\n",
    "                print('Missing')\n",
    "            \n",
    "            # Get data before and after the missing timestamp\n",
    "            before_data = data_log_return[data_log_return.index < timestamp].tail(5)  # 5 data points before\n",
    "            after_data = data_log_return[data_log_return.index > timestamp].head(5)  # 5 data points after\n",
    "            \n",
    "            # Display surrounding data\n",
    "            if not before_data.empty:\n",
    "                print(\"\\nData before:\")\n",
    "                print(before_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available before the missing timestamp.\")\n",
    "            \n",
    "            if not after_data.empty:\n",
    "                print(\"\\nData after:\")\n",
    "                print(after_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available after the missing timestamp.\")\n",
    "\n",
    "    sequences = created_sequences_2(data_log_return, sequence_length, sliding_interval)\n",
    "\n",
    "    # Split sequences into training, validation, and test sets\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    val_size = int(len(sequences) * 0.1)\n",
    "\n",
    "    train_sequences = sequences[:train_size]\n",
    "    val_sequences = sequences[train_size:train_size + val_size]\n",
    "    test_sequences = sequences[train_size + val_size:]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Number of sequences:\n",
    "        - sequences[0].shape: {sequences[0].shape}\n",
    "        - Total sequences: {len(sequences)}\n",
    "        - Train sequences: {len(train_sequences)}\n",
    "        - Validation sequences: {len(val_sequences)}\n",
    "        - Test sequences: {len(test_sequences)}\n",
    "    \"\"\")\n",
    "\n",
    "    # Process train, validation, and test sets\n",
    "    X_train, y_train = split_X_y(train_sequences, \n",
    "                                target_column='trend',\n",
    "                                detect_trends_function = detect_trends_4,\n",
    "                                column= 'close',\n",
    "                                lower_threshold=lower_threshold, \n",
    "                                upper_threshold=upper_threshold, \n",
    "                                reverse_steps=reverse_steps,\n",
    "                                trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_val, y_val = split_X_y(val_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_test, y_test = split_X_y(test_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_train):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_train at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_train):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_train at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_val):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_val at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_val):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_val at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_test):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_test at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_test):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_test at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    if isinstance(y_train, np.ndarray) and y_train.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_train = np.array(y_train, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_val, np.ndarray) and y_val.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_val = np.array(y_val, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_test, np.ndarray) and y_test.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_test = np.array(y_test, dtype=np.int64)\n",
    "\n",
    "    close_col_index = data_log_return.columns.get_loc('close') # 'date' is set as index so doesnt count as a column\n",
    "    Number_features = X_train.shape[-1]\n",
    "    print(f\"close_col_index = {close_col_index}\")\n",
    "    print(f\"Number_features = {Number_features}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n",
    "\n",
    "def print_class_distribution(y, var_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints the class distribution of a label array.\n",
    "\n",
    "    Args:\n",
    "        y: Tensor, array, or list of class labels.\n",
    "        var_name: Name of the variable (for display).\n",
    "    \"\"\"\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.cpu().numpy()\n",
    "    flattened = np.array(y).flatten()\n",
    "\n",
    "    unique_classes, counts = np.unique(flattened, return_counts=True)\n",
    "    total = counts.sum()\n",
    "\n",
    "    header = f\"Class Distribution for '{var_name}':\"\n",
    "    line_parts = [\n",
    "        f\"Class {int(c):<3} Percent: {(count / total) * 100:>6.2f}%\"\n",
    "        for c, count in zip(unique_classes, counts)\n",
    "    ]\n",
    "    print(header.ljust(40) + \" || \".join(line_parts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All(Initial) parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'BTC-USD'\n",
    "downsampled_data_minutes = 1\n",
    "\n",
    "# Step 0 (Again): Identify parameters for trend settings of the loaded data with 1,000 data points\n",
    "lower_threshold = 0.0009 \n",
    "upper_threshold = 0.015\n",
    "reverse_steps = 13\n",
    "\n",
    "\n",
    "exclude_columns= ['MACD', 'MACD_signal', 'ROC_10', 'OBV', 'AD_Line']\n",
    "# exclude_columns= []#['open', 'high', 'low', 'MACD', 'MACD_signal', 'BB_middle', 'ROC_10', 'OBV', 'AD_Line']\n",
    "\n",
    "# Step 3, under ### Correlation Analysis\n",
    "# Compute correlations with the 'trend' column\n",
    "# corr = data_trends.corr()\n",
    "# trend_corr = corr['trend'].sort_values(ascending=False)\n",
    "strongly_correlated = ['close', 'open', 'SMA_5', 'high', 'low', 'EMA_10', 'SMA_10'] # Strongly correlated (correlation > 0.6)\n",
    "moderately_correlated = ['BB_middle', 'BB_lower', 'BB_upper', 'RSI_14'] # Moderately correlated (correlation between 0.3 and 0.6)\n",
    "weakly_correlated = ['SMA_50', 'volume', 'BBW', 'ATR_14'] # Weakly correlated or negligible (correlation <~ 0.3)\n",
    "\n",
    "exclude_columns += weakly_correlated + moderately_correlated\n",
    "# print(exclude_columns)\n",
    "\n",
    "\n",
    "sequence_length = 1000\n",
    "sliding_interval = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Build the GRU Model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 20 11:56:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.02                 Driver Version: 576.02         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   32C    P8              4W /  104W |    1474MiB /   8188MiB |     13%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4004    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A            4540    C+G   ...x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "|    0   N/A  N/A            6588    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            6692    C+G   ...s\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A            8244    C+G   ...2p2nqsd0c76g0\\app\\ChatGPT.exe      N/A      |\n",
      "|    0   N/A  N/A           10416    C+G   ...__kzh8wxbdkxb8p\\DCv2\\DCv2.exe      N/A      |\n",
      "|    0   N/A  N/A           11464    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           12216    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           13772    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15144    C+G   ....0.3485.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           17468    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18596    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           19340    C+G   ...s\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A           19400    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           19856    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           24600    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           27060    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           27272    C+G   ...s\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A           28520    C+G   ...4__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A           29008    C+G   ...__8she8kybcnzg4\\app\\Slack.exe      N/A      |\n",
      "|    0   N/A  N/A           29744    C+G   ...__8she8kybcnzg4\\app\\Slack.exe      N/A      |\n",
      "|    0   N/A  N/A           31840    C+G   ...lus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A           37464    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           41060    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           41712    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# If not in Jupyter Notebook\n",
    "# import subprocess\n",
    "# result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch and CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() True\n",
      "\n",
      "GPU Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "PyTorch compiled CUDA version: 12.4\n",
      "Number of GPUs: 1\n",
      "Total CUDA Cores: 3072\n",
      "Current GPU Device: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "print(\"torch.cuda.is_available()\", gpu_available)\n",
    "\n",
    "# If GPU is available, print additional information\n",
    "if gpu_available:\n",
    "    print(\"\\nGPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"PyTorch compiled CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Total CUDA Cores:\", torch.cuda.get_device_properties(0).multi_processor_count * 128)  # NVIDIA GPUs often have 128 cores/SM\n",
    "    print(\"Current GPU Device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Seeding successful!\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed=42)\n",
    "print(\"Seeding successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_period_files_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llm_timer-series-labeling-artifacts\\\\Data\\\\Polygon_BTCUSD_4Y_1min\\\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'BTCUSD'\n",
    "file_name = f'Polygon_{pair}_4Y_1min'  # File name for saving data\n",
    "# BASE_FOLDER_PATH = f\"{Working_directory}/Data/{file_name}\"\n",
    "BASE_FOLDER_PATH = f\"llm_timer-series-labeling-artifacts/Data/{file_name}\"\n",
    "# folder_path=f'{BASE_FOLDER_PATH}/polygon_io/12_USD_Crypto_Pairs/{file_name}'\n",
    "folder_path=f'{BASE_FOLDER_PATH}'\n",
    "if not os.path.isdir(folder_path):\n",
    "    raise FileNotFoundError(f\"Directory '{folder_path}' does not exist.\")\n",
    "file_path=f'{folder_path}/{file_name}.csv'\n",
    "number_days = 190\n",
    "with_indicators_file_path = os.path.normpath(f'{folder_path}/_{file_name}_{number_days}_days_with_indicators.csv')\n",
    "\n",
    "# LABORATORY\\_Global_Pytorch\\Continual_Learning\\Data\\Polygon_BTCUSD_4Y_1min\n",
    "\n",
    "list_period_files_full_path = [\n",
    "    # Period 1\n",
    "    with_indicators_file_path,\n",
    "    # Period 2\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\"),\n",
    "    # Period 3\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\"),\n",
    "    # Period 4\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\"),\n",
    "    # Period 5\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\")\n",
    "]\n",
    "\n",
    "with_indicators_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: Polygon_BTCUSD_4Y_1min.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_525_days.png\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days.png\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(folder_path):\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File path: Data\\Polygon_BTCUSD_4Y_1min\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n",
      "\n",
      "data_retrieved - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "data_gaussian - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "columns_to_transform = \n",
      "Index(['open', 'high', 'low', 'close', 'SMA_5', 'SMA_10', 'EMA_10'], dtype='object'), \n",
      "len(columns_to_transform) = 7\n",
      "\n",
      "data_log_return - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "No missing timestamps.\n",
      "\n",
      "    Number of sequences:\n",
      "        - sequences[0].shape: (1000, 7)\n",
      "        - Total sequences: 4543\n",
      "        - Train sequences: 3634\n",
      "        - Validation sequences: 454\n",
      "        - Test sequences: 455\n",
      "    \n",
      "close_col_index = 3\n",
      "Number_features = 7\n",
      "unique_classes = [0 1 2 3 4]\n",
      "num_classes = 5\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "    with_indicators_file_path = list_period_files_full_path[0],\n",
    "    downsampled_data_minutes = downsampled_data_minutes,\n",
    "    exclude_columns = exclude_columns,\n",
    "    lower_threshold = lower_threshold,\n",
    "    upper_threshold = upper_threshold,\n",
    "    reverse_steps = reverse_steps,\n",
    "    sequence_length = sequence_length,\n",
    "    sliding_interval = sliding_interval,\n",
    "    trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n",
    "del unique_classes, num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __*All periods data*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n- 'trend': Categorized trend values based on the detected phases:\\n    - 0: No trend\\n    - 1: Moderate negative trend\\n    - 2: Very strong negative trend\\n    - 3: Moderate positive trend\\n    - 4: Very strong positive trend\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 'trend': Categorized trend values based on the detected phases:\n",
    "    - 0: No trend\n",
    "    - 1: Moderate negative trend\n",
    "    - 2: Very strong negative trend\n",
    "    - 3: Moderate positive trend\n",
    "    - 4: Very strong positive trend\n",
    "\"\"\"\n",
    "\n",
    "# Initialize empty lists to combine validation elements\n",
    "# X_train_all = []\n",
    "# y_train_all = []\n",
    "# X_val_all = []\n",
    "# y_val_all = []\n",
    "# X_test_all = []\n",
    "# y_test_all = []\n",
    "\n",
    "# for path_ in list_period_files_full_path:\n",
    "#     with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#         X_train_, y_train_, X_val_, y_val_, X_test_, y_test_, Number_features = process_and_return_splits(\n",
    "#             with_indicators_file_path = path_,\n",
    "#             downsampled_data_minutes = downsampled_data_minutes,\n",
    "#             exclude_columns = exclude_columns,\n",
    "#             lower_threshold = lower_threshold,\n",
    "#             upper_threshold = upper_threshold,\n",
    "#             reverse_steps = reverse_steps,\n",
    "#             sequence_length = sequence_length,\n",
    "#             sliding_interval = sliding_interval,\n",
    "#             trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "#         )\n",
    "#     # Combine validation elements\n",
    "#     X_train_all.extend(X_train_)\n",
    "#     y_train_all.extend(y_train_)\n",
    "#     X_val_all.extend(X_val_)\n",
    "#     y_val_all.extend(y_val_)\n",
    "#     X_test_all.extend(X_test_)\n",
    "#     y_test_all.extend(y_test_)\n",
    "#     # Delete unused variables to save memory\n",
    "#     del X_train_, y_train_, X_val_, y_val_, X_test_, y_test_\n",
    "\n",
    "# X_train_all = np.array(X_train_all)\n",
    "# y_train_all = np.array(y_train_all)\n",
    "# X_val_all = np.array(X_val_all)\n",
    "# y_val_all = np.array(y_val_all)\n",
    "# X_test_all = np.array(X_test_all)\n",
    "# y_test_all = np.array(y_test_all)\n",
    "\n",
    "\n",
    "# with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "#         with_indicators_file_path = list_period_files_full_path[0],\n",
    "#         downsampled_data_minutes = downsampled_data_minutes,\n",
    "#         exclude_columns = exclude_columns,\n",
    "#         lower_threshold = lower_threshold,\n",
    "#         upper_threshold = upper_threshold,\n",
    "#         reverse_steps = reverse_steps,\n",
    "#         sequence_length = sequence_length,\n",
    "#         sliding_interval = sliding_interval,\n",
    "#         trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "#         # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "#     )\n",
    "    \n",
    "# print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "# unique_classes = np.unique(y_val)\n",
    "# num_classes = len(unique_classes)\n",
    "# print(f\"unique_classes = {unique_classes}\")\n",
    "# print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period 1 --> Training and saving in __*'1st_try'*__ (BiGRUWithAttention, num_layers = 4) ---> Val acc = 98.35 %\n",
    "### Val-Class-Acc: {0: '98.63%', 1: '97.92%'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number_features = 7\n",
      "unique_classes = [0 1]\n",
      "num_classes = 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 'trend': Categorized trend values based on the detected phases:\n",
    "    - 0: No trend\n",
    "    - 1: Moderate negative trend\n",
    "    - 2: Very strong negative trend\n",
    "    - 3: Moderate positive trend\n",
    "    - 4: Very strong positive trend\n",
    "\"\"\"\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[0], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "        # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
