{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *__Working on BTCUSD predictions with GRU model.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *__Check first before starting__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Do 'pipenv install ipykernel' if you get error.\n",
    "print(\"Kernel is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\james\\OneDrive\\文件\\Continual_Learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the working directory to the project root\n",
    "# Working_directory = os.path.normpath(\"C:/Users/gilda/OneDrive/Documents/_NYCU/MASTER_S_studies/Master\\'s Thesis/LABORATORY/_Global_Pytorch/Continual_Learning\")\n",
    "Working_directory = os.path.normpath(\"C:/Users/james/OneDrive/文件/Continual_Learning\")\n",
    "os.chdir(Working_directory)\n",
    "print(f\"Working directory: {os.getcwd()}\")  # Prints the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **__All imports__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_interactions import zoom_factory, panhandler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from ta import trend, momentum, volatility, volume\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from typing import Callable, Tuple\n",
    "import shutil\n",
    "import contextlib\n",
    "import traceback\n",
    "import gc\n",
    "import glob, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __**All functions (For data processing)**__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def plot_with_matplotlib(data: pd.DataFrame, \n",
    "                         title: str, \n",
    "                         interactive: bool = False, \n",
    "                         save_path: str = None, \n",
    "                         show_plot: bool = True, \n",
    "                         save_matplotlib_object: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot data using Matplotlib, with optional interactivity using mpl-interactions.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The data to plot, must contain 'close' column.\n",
    "    - title (str): The title of the plot.\n",
    "    - interactive (bool): If True, enables interactive zoom and pan.\n",
    "    - save_path (Optional[str]): If provided, saves the plot to this path.\n",
    "    - show_plot (bool): If True, displays the plot. If False, skips display.\n",
    "    - save_matplotlib_object (Optional[str]): If provided, saves the Matplotlib figure object to this file path.\n",
    "    \"\"\"\n",
    "    if not all(col in data.columns for col in ['close']):\n",
    "        raise ValueError(\"The input DataFrame must contain 'close' column.\")\n",
    "\n",
    "    # Use the default Matplotlib color cycle for the line\n",
    "    default_blue = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "    print(f\"Default blue: {default_blue}\")\n",
    "\n",
    "    # Explicit colors for trends\n",
    "    trend_colors = {\n",
    "        0: 'black',\n",
    "        1: 'yellow',\n",
    "        2: 'red',\n",
    "        3: 'green',\n",
    "        4: default_blue #'purple',\n",
    "    }\n",
    "    # unique_trends = [0, -25, -15, 15, 25]\n",
    "    # colormap = plt.cm.get_cmap('tab10', len(unique_trends))  # Choose 'tab10' or 'Set1' for distinct colors\n",
    "    # trend_colors = {trend: colormap(i) for i, trend in enumerate(unique_trends)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot data as a single connected line, colored by trend\n",
    "    if 'trend' in data.columns:\n",
    "        legend_added = set() # Track which trends have already been added to the legend\n",
    "        prev_idx = data.index[0]\n",
    "        for idx, row in data.iterrows():\n",
    "            if idx != prev_idx:\n",
    "                trend_key = int(row['trend'])  # Convert trend value to int for lookup\n",
    "                label = f'Trend {trend_key}' if trend_key not in legend_added else None\n",
    "                ax.plot([prev_idx, idx], \n",
    "                        [data.loc[prev_idx, 'close'], row['close']],\n",
    "                        color=trend_colors[trend_key], \n",
    "                        linestyle='-', \n",
    "                        # marker='o', \n",
    "                        linewidth=1,\n",
    "                        label=label  # Add label only if it's not in the legend\n",
    "                )\n",
    "                legend_added.add(trend_key)  # Mark this trend as added to the legend\n",
    "            prev_idx = idx\n",
    "\n",
    "        ax.set_title(f\"{title} (Connected, Colored by Trend)\")\n",
    "    else:\n",
    "        # Default plot if no trend column exists\n",
    "        ax.plot(data.index, data['close'], label='Closing Price', linestyle='-', marker='o', \n",
    "                markersize=2, linewidth=1, color=default_blue, markerfacecolor='green', markeredgecolor='black')\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing Price (USD)')\n",
    "    \n",
    "    # Add a legend manually for trends\n",
    "    # for trend, color in trend_colors.items():\n",
    "    #     ax.plot([], [], color=color, label=f'Trend {trend}')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    # Enable interactivity if requested\n",
    "    if interactive:\n",
    "        zoom_factory(ax)  # Enable zoom with mouse wheel\n",
    "        panhandler(fig)   # Enable panning with left-click\n",
    "        print(\"Interactive mode enabled. Use mouse wheel to zoom and left click to pan.\")\n",
    "\n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        fig.tight_layout()  # Ensures the layout is clean\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "\n",
    "    # Save the Matplotlib figure object\n",
    "    if save_matplotlib_object:\n",
    "        with open(save_matplotlib_object, 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "        print(f\"Matplotlib figure object saved to: {save_matplotlib_object}\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plot display skipped.\")\n",
    "\n",
    "def load_and_show_pickle(pickle_file_path: str):\n",
    "    \"\"\"\n",
    "    Load a pickled Matplotlib figure object and display it with optional interactivity.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_file_path (str): Path to the pickled Matplotlib figure file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the pickle file and load the figure\n",
    "        with open(pickle_file_path, \"rb\") as f:\n",
    "            loaded_fig = pickle.load(f)\n",
    "\n",
    "        print(f\"Figure successfully loaded and displayed from: {pickle_file_path}\")\n",
    "\n",
    "        # Use plt.show() to allow interactivity\n",
    "        plt.show(block=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pickle_file_path}. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the pickled figure: {e}\")\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data to be saved.\n",
    "        file_path (str): The file path (including the file name) to save the CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path)\n",
    "    # df_to_save = df.copy()\n",
    "    # df_to_save[\"date\"] = df_to_save.index.strftime('%Y-%m-%d %H:%M:%S')  # Add formatted index as a column\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    # df_to_save.to_csv(file_path)\n",
    "    print(f\"\\nSuccessfully saved data with moving average to CSV: \\n\\t{file_path}\\n\")\n",
    "\n",
    "def read_csv_file(file_path: str, preview_rows: int = 5, \n",
    "                  days_towards_end: int = None, \n",
    "                  days_from_start: int = None, description: str = \"\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a pandas DataFrame filtered by date range.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        preview_rows (int): Number of rows to preview (default is 5).\n",
    "        days_towards_end (int, optional): Number of days from the most recent date to retrieve data.\n",
    "        days_from_start (int, optional): Number of days from the oldest date of the filtered data to retrieve data.\n",
    "        description (str): A brief description of the dataset being loaded.\n",
    "                           Explanation:\n",
    "                           - To retrieve data from the **end**: Use `days_towards_end`.\n",
    "                           - To retrieve data from the **start of the filtered range**: Use `days_from_start`.\n",
    "                           - To retrieve data from the **middle**: Use both:\n",
    "                             For example, if `days_towards_end=100` and `days_from_start=50`,\n",
    "                             the function will first filter the last 100 days of the dataset,\n",
    "                             and then filter the first 50 days from this range.\n",
    "                             This results in data between the last 100th and the last 50th day.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The loaded and filtered data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if description:\n",
    "            print(f\"\\nDescription: {description}\")\n",
    "        print(f\"\\nFile path: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
    "        \n",
    "        # Filter by days towards the end\n",
    "        if days_towards_end is not None:\n",
    "            last_date = data.index.max()  # Get the most recent date in the dataset\n",
    "            end_cutoff_date = last_date - pd.Timedelta(days=days_towards_end)\n",
    "            data = data[data.index >= end_cutoff_date]\n",
    "            print(f\"\\nRetrieving data from the past {days_towards_end} days (from {end_cutoff_date.date()} onwards):\")\n",
    "        \n",
    "        # Filter by days from the start (from the filtered data)\n",
    "        if days_from_start is not None:\n",
    "            first_date = data.index.min()  # Get the earliest date in the filtered dataset\n",
    "            start_cutoff_date = first_date + pd.Timedelta(days=days_from_start)\n",
    "            data = data[data.index <= start_cutoff_date]\n",
    "            print(f\"\\nRetrieving the first {days_from_start} days from the filtered data (up to {start_cutoff_date.date()}):\")\n",
    "\n",
    "        if preview_rows:\n",
    "            # Print a preview of the data\n",
    "            print(f\"\\nPreview of the first {preview_rows} rows:\")\n",
    "            # print(data.head(preview_rows), '\\n')\n",
    "            display(data.head(preview_rows))\n",
    "            print()\n",
    "\n",
    "            print(f\"\\nPreview of the last {preview_rows} rows:\")\n",
    "            # print(data.tail(preview_rows), '\\n')\n",
    "            display(data.tail(preview_rows))\n",
    "            print()\n",
    "\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The file could not be parsed. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def downsample_minute_data(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsample minute data into N-minute intervals by retaining every Nth row.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The original DataFrame with a datetime index.\n",
    "        n (int): The number of minutes for the downsampling interval.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Downsampled DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n========---> Downsampling the data! \\n\")\n",
    "    data = data.copy()\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            data.index = pd.to_datetime(data.index)  # Convert to DatetimeIndex\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"The DataFrame index could not be converted to DatetimeIndex.\") from e\n",
    "\n",
    "    # Filter rows where the minute index modulo N is 0\n",
    "    downsampled_data = data[data.index.minute % n == 0]\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "def calculate_log_returns_all_columns(data: pd.DataFrame, exclude_columns: list = [], dropna: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate log returns for all numeric columns in a pandas DataFrame,\n",
    "    excluding specified columns, and removing excluded columns from the returned DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame containing numeric data.\n",
    "        exclude_columns (list): List of columns to exclude from log return calculations and the result.\n",
    "        dropna (bool): Whether to drop rows with NaN values resulting from the calculation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with log returns for numeric columns,\n",
    "                      excluding specified columns.\n",
    "    \"\"\"\n",
    "    data = data.copy().drop(columns=exclude_columns)\n",
    "    columns_to_transform = data.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"columns_to_transform = \\n{columns_to_transform}, \\nlen(columns_to_transform) = {len(columns_to_transform)}\")\n",
    "\n",
    "    for col in columns_to_transform:\n",
    "        # Ensure no negative or zero values\n",
    "        if (data[col] <= 0).any():\n",
    "            raise ValueError(f\"Column '{col}' contains non-positive values. Log returns require strictly positive values.\")\n",
    "        data[col] = np.log(data[col] / data[col].shift(1))\n",
    "\n",
    "    # Optionally drop rows with NaN values\n",
    "    return data.dropna() if dropna else data\n",
    "\n",
    "def created_sequences_2(data: pd.DataFrame, sequence_length: int = 60, sliding_interval: int = 60) -> list:\n",
    "    \"\"\"\n",
    "    Divide the dataset into sequences based on the sequence_length.\n",
    "    Each sequence must fully cover the window size.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): The input DataFrame.\n",
    "    - sequence_length (int): The window size for sequences.\n",
    "\n",
    "    Returns:\n",
    "    - sequences (list): A list of sequences (as DataFrames).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, sliding_interval):\n",
    "        # print(f\"Processing sequence starting at index: {i}\")\n",
    "        seq = data.iloc[i:i + sequence_length].copy()\n",
    "        sequences.append(seq)\n",
    "    # print(f\"Total sequences created: {len(sequences)}\")\n",
    "    return sequences\n",
    "\n",
    "def gaussian_smoothing(data: pd.DataFrame, sigma=2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to numeric columns in a DataFrame and ensures the index is sorted in ascending order.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        sigma (float): Standard deviation for the Gaussian kernel. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with smoothed numeric columns and sorted index.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by index in ascending order\n",
    "    data = data.sort_index(ascending=True).copy()\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):  # Only apply to numeric columns\n",
    "            data[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    return data\n",
    "\n",
    "def detect_trends_4(\n",
    "    dataframe: pd.DataFrame, \n",
    "    column: str = 'close', \n",
    "    lower_threshold: float = 0.001, \n",
    "    upper_threshold: float = 0.02,\n",
    "    reverse_steps: int = 7,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects trends based on log return data provided in a specified column and categorizes them into different strength levels.\n",
    "\n",
    "    This function analyzes time-series data by evaluating cumulative trends in log return values provided in the input DataFrame. \n",
    "    It uses three dictionaries (`dic1`, `dic2`, `dic3`) to track different phases of trends, handles multi-step reversals, and \n",
    "    classifies trends dynamically based on cumulative product thresholds and specified thresholds for trend strengths.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame containing log return data.\n",
    "        column (str): Column name containing log return values. Defaults to 'close'.\n",
    "        lower_threshold (float): Threshold for categorizing moderate trends. Defaults to 0.001.\n",
    "        upper_threshold (float): Threshold for categorizing strong trends. Defaults to 0.02.\n",
    "        reverse_steps (int): Number of consecutive steps to confirm a trend reversal. Defaults to 7.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added column:\n",
    "                    - 'trend': Categorized trend values based on the detected phases:\n",
    "                        - 0: No trend\n",
    "                        - 1: Moderate negative trend\n",
    "                        - 2: Very strong negative trend\n",
    "                        - 3: Moderate positive trend\n",
    "                        - 4: Very strong positive trend\n",
    "                      Any trends not included in `trends_to_keep` will be reset to 0.\n",
    "\n",
    "    Function Details:\n",
    "    1. **Input Assumption**:\n",
    "    - The input DataFrame already contains log return data in the specified column (`column`).\n",
    "\n",
    "    2. **Trend Tracking**:\n",
    "    - Uses dictionaries to monitor trends:\n",
    "        - `dic1`: Tracks the first phase of the trend.\n",
    "        - `dic2`: Tracks the second phase if a reversal occurs.\n",
    "        - `dic3`: Tracks the third phase if another reversal occurs.\n",
    "\n",
    "    3. **Cumulative Product**:\n",
    "    - Calculates the cumulative product of `(1 + log_return)` from the specified column to evaluate the strength of trends.\n",
    "\n",
    "    4. **Reversal Handling**:\n",
    "    - If a trend reversal persists beyond `reverse_steps`, labels are assigned based on the cumulative product tracked in `dic1`.\n",
    "    - Subsequent reversals are merged or labeled independently if conditions are met.\n",
    "\n",
    "    5. **Label Assignment**:\n",
    "    - Labels are dynamically assigned based on cumulative product thresholds for positive and negative trends:\n",
    "        - Positive trends are categorized as moderate (3) or strong (4).\n",
    "        - Negative trends are categorized as moderate (1) or strong (2).\n",
    "\n",
    "    6. **Trend Filtering**:\n",
    "    - After detecting trends, only those specified in `trends_to_keep` remain unchanged.\n",
    "    - Any trend category not included in `trends_to_keep` is reset to 0 (No Trend).\n",
    "\n",
    "    7. **Edge Cases**:\n",
    "    - Properly handles scenarios where data points are insufficient for trend analysis or when trend phases overlap, ensuring all data points are labeled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    df = dataframe.copy()\n",
    "    df['trend'] = None  # Default value \n",
    "    \n",
    "    # print(\"\\n#-------------------- Working on 'trend' patterns -----------------------#\")\n",
    "    dic1, dic2, dic3 = None, None, None # Initialize trend tracking dictionaries\n",
    "    # dic1 = None # {'ids': [], 'last_sign': None, 'cumulative': 1.0}\n",
    "    \n",
    "    def assign_label(dictio_, lower_threshold, upper_threshold):\n",
    "        cumulative = dictio_['cumulative']\n",
    "        # print(f\"cumulative = {cumulative}\")\n",
    "        if cumulative > (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 4  # Very strong positive\n",
    "        elif (1 + lower_threshold) < cumulative <= (1 + upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 3  # Moderate positive\n",
    "        elif (1 - upper_threshold) < cumulative <= (1 - lower_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 1  # Moderate negative\n",
    "        elif cumulative <= (1 - upper_threshold):\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 2  # Very strong negative\n",
    "        else:\n",
    "            df.iloc[dictio_['ids'], df.columns.get_loc('trend')] = 0  # No trend\n",
    "    \n",
    "    #----------------------- For Loop -----------------------#\n",
    "    for idx, log_ret in enumerate(df[column]):\n",
    "        sign = 1 if log_ret > 0 else -1\n",
    "\n",
    "        if dic1 is None:  # Initialize dic1\n",
    "            # print(f\"\\nThis one time condition 'if loop' is running \\n\")\n",
    "            dic1 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic1['last_sign']\n",
    "        if sign == last_sign and dic2 is None:  # Continue same trend\n",
    "            dic1['ids'].append(idx)\n",
    "            dic1['last_sign'] = sign\n",
    "            dic1['cumulative'] *= (1 + log_ret)\n",
    "            continue\n",
    "\n",
    "        # 1st Reversal occuring\n",
    "        if dic2 is None:  # Start dic2\n",
    "            dic2 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic2['last_sign']\n",
    "        if sign == last_sign and dic3 is None:  # Continue same trend\n",
    "            dic2['ids'].append(idx)\n",
    "            dic2['last_sign'] = sign\n",
    "            dic2['cumulative'] *= (1 + log_ret)\n",
    "            if len(dic2['ids']) == reverse_steps:\n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                # print(f\"dic1['cumulative'] = {dic1['cumulative']}, and dic1['ids'] = {dic1['ids']}\")\n",
    "                dic1 = dic2\n",
    "                dic2 = None\n",
    "                # print(f\"dic1 after trend reversal persisted and dic1 = dic2 = \\n{dic1}\")\n",
    "                # print(f\"dic2 after being reset: {dic2}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 2nd Reversal occuring\n",
    "        if dic3 is None:  # Start dic3\n",
    "            dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "            continue\n",
    "        last_sign = dic3['last_sign']\n",
    "        if sign == last_sign: # Continue same trend, there is no dic4 to check if is None\n",
    "            dic3['ids'].append(idx)\n",
    "            dic3['last_sign'] = sign\n",
    "            dic3['cumulative'] *= (1 + log_ret)\n",
    "            dic_prod = dic2['cumulative'] * dic3['cumulative']\n",
    "            # if (sign == 1 and dic1['cumulative'] * dic_prod > dic1['cumulative']) or (sign == -1 and dic1['cumulative'] * dic_prod < dic1['cumulative'])):\n",
    "            if (sign == 1 and dic_prod > 1) or (sign == -1 and dic_prod < 1): # More beautiful\n",
    "                # Merge dic1, dic2, and dic3\n",
    "                dic1['ids'] += dic2['ids'] + dic3['ids']\n",
    "                dic1['last_sign'] = dic3['last_sign']\n",
    "                dic1['cumulative'] *= dic2['cumulative'] * dic3['cumulative']\n",
    "                dic2, dic3 = None, None\n",
    "                continue\n",
    "\n",
    "            if len(dic3['ids']) == reverse_steps:      \n",
    "                assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                assign_label(dic2, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "                dic1 = dic3\n",
    "                dic2, dic3 = None, None\n",
    "                # print(f\"dic2 after 2nd trend reversal didn't catch up fast enough, and now \\ndic1 = dic3 = {dic1}\")\n",
    "                # print(f\"dic3 and dic2 after being reset: {dic3}\\n\")\n",
    "            continue\n",
    "            \n",
    "        # 3rd Reversal occuring\n",
    "        assign_label(dic1, lower_threshold, upper_threshold) # Assign labels in the 'trend' column for ids of dic1\n",
    "        # Reassign values\n",
    "        dic1 = dic2\n",
    "        dic2 = dic3\n",
    "        dic3 = {'ids': [idx], 'last_sign': sign, 'cumulative': (1 + log_ret)}\n",
    "        # print(f\"There was a 3rd trend reversal, and now \\ndic1 = dic2 = {dic1}, \\ndic2 = dic3 = {dic2}\")\n",
    "        # print(f\"dic3 after being reset: {dic3}\\n\")\n",
    "\n",
    "    # Assign remaining labels\n",
    "    if dic1:\n",
    "        assign_label(dic1, lower_threshold, upper_threshold)\n",
    "    if dic2:\n",
    "        assign_label(dic2, lower_threshold, upper_threshold)\n",
    "    if dic3:\n",
    "        assign_label(dic3, lower_threshold, upper_threshold)\n",
    "    # print(\"\\n#-------------------- Returning 'trend' patterns ------------------------#\")\n",
    "    \n",
    "    # Apply filtering: Keep only selected trends, set others to 0\n",
    "    df['trend'] = df['trend'].apply(lambda x: x if x in trends_to_keep else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_X_y(sequences: list[pd.DataFrame], \n",
    "              target_column: str = 'trend',\n",
    "              detect_trends_function: Callable[[pd.DataFrame, str, float, float, int, set], pd.DataFrame] = detect_trends_4, \n",
    "              column: str = 'close', \n",
    "              lower_threshold: float = 0.0009, \n",
    "              upper_threshold: float = 0.015,\n",
    "              reverse_steps: int = 7,\n",
    "              trends_to_keep: set = {0, 1, 2, 3, 4}) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process sequences to generate features (X) and labels (y) while applying trend detection.\n",
    "\n",
    "    Args:\n",
    "    - sequences (list of pd.DataFrame): List of DataFrame sequences.\n",
    "    - lower_threshold (float): Lower threshold for trend detection.\n",
    "    - upper_threshold (float): Upper threshold for trend detection.\n",
    "    - reverse_steps (int): Steps to reverse trends in the sequence.\n",
    "    - target_column (str): Column name to use as the label (default: 'trend').\n",
    "    - detect_trends_function (Callable): Function for detecting trends, defaults to `detect_trends_4`.\n",
    "    - trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend).\n",
    "\n",
    "    Returns:\n",
    "    - X (np.ndarray): Features array of shape (num_sequences, sequence_length, num_features).\n",
    "    - y (np.ndarray): Labels array of shape (num_sequences,).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # count = 0\n",
    "    for seq in sequences:\n",
    "        # Apply trend detection on the sequence\n",
    "        seq = detect_trends_function(seq, column=column, \n",
    "                                     lower_threshold=lower_threshold, \n",
    "                                     upper_threshold=upper_threshold, \n",
    "                                     reverse_steps=reverse_steps,\n",
    "                                     trends_to_keep=trends_to_keep)\n",
    "        # if count == 0:\n",
    "        #     count = 1\n",
    "        #     print(f\"\\nseq.head()\")\n",
    "        #     display(seq.head())\n",
    "        #     print()\n",
    "        # Extract features (X) and labels (y)\n",
    "        X.append(seq.drop(columns=[target_column]).values)  # All but the target column\n",
    "        y.append(seq[target_column].values)  # Target column as labels\n",
    "        \n",
    "    # return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def process_and_return_splits(\n",
    "    with_indicators_file_path: str,\n",
    "    downsampled_data_minutes: int,\n",
    "    exclude_columns: list[str],\n",
    "    lower_threshold: float,\n",
    "    upper_threshold: float,\n",
    "    reverse_steps: int,\n",
    "    sequence_length: int,\n",
    "    sliding_interval: int,\n",
    "    trends_to_keep: set = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ") -> tuple[\n",
    "    list[list[float]],  # X_train: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_train: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_val: List of sequences, each containing a list of features\n",
    "    list[list[int]],    # y_val: List of sequences, each containing a list of labels\n",
    "    list[list[float]],  # X_test: List of sequences, each containing a list of features\n",
    "    list[list[int]]     # y_test: List of sequences, each containing a list of labels\n",
    "]:\n",
    "    \"\"\"\n",
    "    Processes time-series data from a CSV file and prepares it for machine learning.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Reads data from the specified CSV file and sorts it by date in descending order.\n",
    "        2. Optionally downsamples the data to a lower frequency (e.g., 5-minute intervals).\n",
    "        3. Applies Gaussian smoothing to reduce noise in the data.\n",
    "        4. Calculates log returns for all numeric columns, excluding specified columns.\n",
    "        5. Detects trends based on defined thresholds (`lower_threshold`, `upper_threshold`, and `reverse_steps`).\n",
    "        6. Filters trends to keep only those specified in `trends_to_keep`, setting others to 0 (No Trend).\n",
    "        7. Converts the processed data into sequences of a fixed length (`sequence_length`) with a sliding interval.\n",
    "        8. Splits the sequences into training (80%), validation (10%), and test (10%) sets.\n",
    "        9. Further splits the sequences into features (`X`) and labels (`y`) for supervised learning.\n",
    "\n",
    "    Args:\n",
    "        with_indicators_file_path (str): Path to the CSV file containing the time-series data.\n",
    "        downsampled_data_minutes (int): Frequency for downsampling the data (e.g., 1 for no downsampling).\n",
    "        exclude_columns (list[str]): List of column names to exclude from log return calculations.\n",
    "        lower_threshold (float): Lower threshold for trend detection.\n",
    "        upper_threshold (float): Upper threshold for trend detection.\n",
    "        reverse_steps (int): Number of steps for reversing trends in trend detection.\n",
    "        sequence_length (int): Length of sequences to create from the data.\n",
    "        sliding_interval (int): Interval for sliding the window when creating sequences.\n",
    "        trends_to_keep (set): A set of trend categories to retain; others will be set to 0 (No Trend). Defaults to keeping all trends {0, 1, 2, 3, 4}.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[list[float]], list[list[int]], list[list[float]], list[list[int]], list[list[float]], list[list[int]]]:\n",
    "            A tuple containing:\n",
    "            - X_train (list[list[float]]): Input sequences for training.\n",
    "            - y_train (list[list[int]]): Target sequences for training.\n",
    "            - X_val (list[list[float]]): Input sequences for validation.\n",
    "            - y_val (list[list[int]]): Target sequences for validation.\n",
    "            - X_test (list[list[float]]): Input sequences for testing.\n",
    "            - y_test (list[list[int]]): Target sequences for testing.\n",
    "\n",
    "    Example:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = process_and_return_splits(\n",
    "            with_indicators_file_path=\"data.csv\",\n",
    "            downsampled_data_minutes=5,\n",
    "            exclude_columns=[\"volume\"],\n",
    "            lower_threshold=-0.05,\n",
    "            upper_threshold=0.05,\n",
    "            reverse_steps=3,\n",
    "            sequence_length=50,\n",
    "            sliding_interval=5,\n",
    "            trends_to_keep={1, 2, 3, 4}  # Only keep categorized trends, set others to 0\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    data_retrieved = read_csv_file(with_indicators_file_path, preview_rows=0) # 190 days of data\n",
    "    data_retrieved = data_retrieved.sort_index(ascending=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    if downsampled_data_minutes != 1:\n",
    "        print(\"Downsampling the data! \\n\")\n",
    "        data_retrieved = downsample_minute_data(data_retrieved, downsampled_data_minutes)\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_retrieved.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_retrieved.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_retrieved.index.tz,\n",
    "    ).difference(data_retrieved.index)\n",
    "    print(f\"\\ndata_retrieved - Missing timestamps time: \\n{missing_timestamps}\") \n",
    "\n",
    "    data_gaussian = gaussian_smoothing(data_retrieved, sigma=7)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_gaussian.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_gaussian.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_gaussian.index.tz,\n",
    "    ).difference(data_gaussian.index)\n",
    "    print(f\"\\ndata_gaussian - Missing timestamps time: \\n{missing_timestamps}\\n\")\n",
    "\n",
    "    data_log_return = calculate_log_returns_all_columns(data_gaussian, exclude_columns=exclude_columns)\n",
    "\n",
    "    # Get missing timestamps\n",
    "    missing_timestamps = pd.date_range(\n",
    "        start=data_log_return.index.min(), # Returns smallest/earliest/oldest date\n",
    "        end=data_log_return.index.max(),\n",
    "        freq='1min',  # Use 'min' for a frequency of 1 minute, '30s' for a frequency of 30 seconds\n",
    "        tz=data_log_return.index.tz,\n",
    "    ).difference(data_log_return.index)\n",
    "    print(f\"\\ndata_log_return - Missing timestamps time: \\n{missing_timestamps}\\n\") \n",
    "\n",
    "    # Check if there are missing timestamps\n",
    "    if missing_timestamps.empty:\n",
    "        print(\"No missing timestamps.\")\n",
    "    else:\n",
    "        for timestamp in missing_timestamps:\n",
    "            print(f\"\\nMissing timestamp: {timestamp}\")\n",
    "            \n",
    "            # Create a placeholder for the missing timestamp\n",
    "            if timestamp not in data_log_return.index:\n",
    "                print('Missing')\n",
    "            \n",
    "            # Get data before and after the missing timestamp\n",
    "            before_data = data_log_return[data_log_return.index < timestamp].tail(5)  # 5 data points before\n",
    "            after_data = data_log_return[data_log_return.index > timestamp].head(5)  # 5 data points after\n",
    "            \n",
    "            # Display surrounding data\n",
    "            if not before_data.empty:\n",
    "                print(\"\\nData before:\")\n",
    "                print(before_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available before the missing timestamp.\")\n",
    "            \n",
    "            if not after_data.empty:\n",
    "                print(\"\\nData after:\")\n",
    "                print(after_data)\n",
    "            else:\n",
    "                print(\"\\nNo data available after the missing timestamp.\")\n",
    "\n",
    "    sequences = created_sequences_2(data_log_return, sequence_length, sliding_interval)\n",
    "\n",
    "    # Split sequences into training, validation, and test sets\n",
    "    train_size = int(len(sequences) * 0.8)\n",
    "    val_size = int(len(sequences) * 0.1)\n",
    "\n",
    "    train_sequences = sequences[:train_size]\n",
    "    val_sequences = sequences[train_size:train_size + val_size]\n",
    "    test_sequences = sequences[train_size + val_size:]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Number of sequences:\n",
    "        - sequences[0].shape: {sequences[0].shape}\n",
    "        - Total sequences: {len(sequences)}\n",
    "        - Train sequences: {len(train_sequences)}\n",
    "        - Validation sequences: {len(val_sequences)}\n",
    "        - Test sequences: {len(test_sequences)}\n",
    "    \"\"\")\n",
    "\n",
    "    # Process train, validation, and test sets\n",
    "    X_train, y_train = split_X_y(train_sequences, \n",
    "                                target_column='trend',\n",
    "                                detect_trends_function = detect_trends_4,\n",
    "                                column= 'close',\n",
    "                                lower_threshold=lower_threshold, \n",
    "                                upper_threshold=upper_threshold, \n",
    "                                reverse_steps=reverse_steps,\n",
    "                                trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_val, y_val = split_X_y(val_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    X_test, y_test = split_X_y(test_sequences, \n",
    "                            target_column='trend',\n",
    "                            detect_trends_function = detect_trends_4,\n",
    "                            column= 'close',\n",
    "                            lower_threshold=lower_threshold, \n",
    "                            upper_threshold=upper_threshold, \n",
    "                            reverse_steps=reverse_steps,\n",
    "                            trends_to_keep=trends_to_keep)\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_train):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_train at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_train):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_train at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_val):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_val at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_val):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_val at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    # Checking X arrays\n",
    "    for idx, seq in enumerate(X_test):  # Loop through sequences\n",
    "        for sub_idx, feature_set in enumerate(seq):  # Loop through data points\n",
    "            for feature_idx, feature in enumerate(feature_set):  # Loop through features\n",
    "                if not isinstance(feature, (float, np.float32)):  # Check each feature\n",
    "                    print(f\"Unexpected type in X_test at sequence {idx}, data point {sub_idx}, feature {feature_idx}: {type(feature)}\")\n",
    "    # Checking y arrays\n",
    "    for idx, seq in enumerate(y_test):  # Loop through sequences\n",
    "        for sub_idx, label in enumerate(seq):  # Loop through data points (labels)\n",
    "            if not isinstance(label, (int, np.int64)):  # Check each label\n",
    "                print(f\"Unexpected type in y_test at sequence {idx}, data point {sub_idx}: {type(label)}\")\n",
    "\n",
    "    if isinstance(y_train, np.ndarray) and y_train.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_train = np.array(y_train, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_val, np.ndarray) and y_val.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_val = np.array(y_val, dtype=np.int64)\n",
    "\n",
    "    if isinstance(y_test, np.ndarray) and y_test.dtype == np.object_:\n",
    "        # Convert to numeric if needed\n",
    "        y_test = np.array(y_test, dtype=np.int64)\n",
    "\n",
    "    close_col_index = data_log_return.columns.get_loc('close') # 'date' is set as index so doesnt count as a column\n",
    "    Number_features = X_train.shape[-1]\n",
    "    print(f\"close_col_index = {close_col_index}\")\n",
    "    print(f\"Number_features = {Number_features}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n",
    "\n",
    "def print_class_distribution(y, var_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints the class distribution of a label array.\n",
    "\n",
    "    Args:\n",
    "        y: Tensor, array, or list of class labels.\n",
    "        var_name: Name of the variable (for display).\n",
    "    \"\"\"\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.cpu().numpy()\n",
    "    flattened = np.array(y).flatten()\n",
    "\n",
    "    unique_classes, counts = np.unique(flattened, return_counts=True)\n",
    "    total = counts.sum()\n",
    "\n",
    "    header = f\"Class Distribution for '{var_name}':\"\n",
    "    line_parts = [\n",
    "        f\"Class {int(c):<3} Percent: {(count / total) * 100:>6.2f}%\"\n",
    "        for c, count in zip(unique_classes, counts)\n",
    "    ]\n",
    "    print(header.ljust(40) + \" || \".join(line_parts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All(Initial) parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'BTC-USD'\n",
    "downsampled_data_minutes = 1\n",
    "\n",
    "# Step 0 (Again): Identify parameters for trend settings of the loaded data with 1,000 data points\n",
    "lower_threshold = 0.0009 \n",
    "upper_threshold = 0.015\n",
    "reverse_steps = 13\n",
    "\n",
    "\n",
    "exclude_columns= ['MACD', 'MACD_signal', 'ROC_10', 'OBV', 'AD_Line']\n",
    "# exclude_columns= []#['open', 'high', 'low', 'MACD', 'MACD_signal', 'BB_middle', 'ROC_10', 'OBV', 'AD_Line']\n",
    "\n",
    "# Step 3, under ### Correlation Analysis\n",
    "# Compute correlations with the 'trend' column\n",
    "# corr = data_trends.corr()\n",
    "# trend_corr = corr['trend'].sort_values(ascending=False)\n",
    "strongly_correlated = ['close', 'open', 'SMA_5', 'high', 'low', 'EMA_10', 'SMA_10'] # Strongly correlated (correlation > 0.6)\n",
    "moderately_correlated = ['BB_middle', 'BB_lower', 'BB_upper', 'RSI_14'] # Moderately correlated (correlation between 0.3 and 0.6)\n",
    "weakly_correlated = ['SMA_50', 'volume', 'BBW', 'ATR_14'] # Weakly correlated or negligible (correlation <~ 0.3)\n",
    "\n",
    "exclude_columns += weakly_correlated + moderately_correlated\n",
    "# print(exclude_columns)\n",
    "\n",
    "\n",
    "sequence_length = 1000\n",
    "sliding_interval = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Build the GRU Model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 20 21:00:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.03                 Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P8             10W /  200W |    4389MiB /  12282MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     18500    C+G   ...43.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     54768    C+G   ...al\\Discord\\app-1.0.9188\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A     67352    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     81344      C   ...s\\Medical_AI_LoRA_py3.12\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     83936    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A     85988    C+G   ...ata\\Local\\LINE\\bin\\current\\LINE.exe      N/A      |\n",
      "|    0   N/A  N/A     90496    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A    103372    C+G   ... Stream\\106.0.4.0\\GoogleDriveFS.exe      N/A      |\n",
      "|    0   N/A  N/A    119500    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A    120292    C+G   ...ta\\Local\\Programs\\Notion\\Notion.exe      N/A      |\n",
      "|    0   N/A  N/A    123408    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A    125500    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A    126332    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A    131424    C+G   ...ograms\\cron-web\\Notion Calendar.exe      N/A      |\n",
      "|    0   N/A  N/A    135136    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A    136672    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A    141588    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A    142900    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A    145764    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A    146092    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A    149096      C   ...s\\Medical_AI_LoRA_py3.12\\python.exe      N/A      |\n",
      "|    0   N/A  N/A    150348    C+G   ...t Office\\root\\Office16\\POWERPNT.EXE      N/A      |\n",
      "|    0   N/A  N/A    151976      C   ...s\\Medical_AI_LoRA_py3.12\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# If not in Jupyter Notebook\n",
    "# import subprocess\n",
    "# result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "# print(result.stdout)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch and CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() True\n",
      "\n",
      "GPU Device Name: NVIDIA GeForce RTX 4070\n",
      "Number of GPUs: 1\n",
      "Total CUDA Cores: 5888\n",
      "Current GPU Device: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "print(\"torch.cuda.is_available()\", gpu_available)\n",
    "\n",
    "# If GPU is available, print additional information\n",
    "if gpu_available:\n",
    "    print(\"\\nGPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Total CUDA Cores:\", torch.cuda.get_device_properties(0).multi_processor_count * 128)  # NVIDIA GPUs often have 128 cores/SM\n",
    "    print(\"Current GPU Device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Directional GRU with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUWithAttention(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int, dropout: float = 0.0):\n",
    "        super(BiGRUWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Bi-Directional GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        # Attention layer\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)  # Hidden size * 2 for bi-directional\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)  # Apply dropout before the fully connected layer\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Xavier initialization for weights\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)  # Zero initialization for biases\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Bi-directional: num_layers * 2\n",
    "        # Bi-Directional GRU forward pass\n",
    "        out, _ = self.gru(x, h0)  # Shape: (batch_size, seq_length, hidden_size * 2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.tanh(self.attention_fc(out))  # Shape: (batch_size, seq_length, hidden_size * 2)\n",
    "        out = attn_weights * out  # Element-wise attention application\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "\n",
    "        # Fully connected layer (applied at each time step)\n",
    "        out = self.fc(out)  # Shape: (batch_size, seq_length, output_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Architecture with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Layer ---\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=4):\n",
    "        \"\"\"\n",
    "        in_features: Input features\n",
    "        out_features: Output features (number of classes for classification)\n",
    "        r: Rank of the low-rank update matrices\n",
    "        \"\"\"\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Original linear layer (frozen during incremental training)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Low-rank adaptation parameters\n",
    "        self.r = r\n",
    "        self.A = nn.Parameter(torch.randn(r, in_features) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(out_features, r) * 0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute the LoRA update: W' = W + (B @ A)\n",
    "        lora_update = F.linear(x, self.B @ self.A)\n",
    "        orig_output = self.linear(x)\n",
    "        return orig_output + lora_update\n",
    "\n",
    "# --- New Model Architecture ---\n",
    "class BiGRUWithAttentionLoRA(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int, dropout: float = 0.0, lora_r: int = 4):\n",
    "        super(BiGRUWithAttentionLoRA, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bi-directional GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        # Attention layer\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # Fully connected layer replaced by a LoRA adapted linear layer\n",
    "        self.fc = LoRALinear(hidden_size * 2, output_size, r=lora_r)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'linear' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Initialize hidden state for bi-directional GRU: (num_layers * 2, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)  # Shape: [batch, seq_len, hidden_size*2]\n",
    "        # Apply attention mechanism\n",
    "        attn_weights = torch.tanh(self.attention_fc(out))\n",
    "        out = attn_weights * out  # Element-wise multiplication\n",
    "        out = self.dropout(out)\n",
    "        # Final prediction for each time step\n",
    "        out = self.fc(out)  # Shape: [batch, seq_len, output_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation function for Period 1.\n",
    "def train_and_validate(model, output_size, criterion, optimizer, \n",
    "                       X_train, y_train, X_val, y_val, scheduler, \n",
    "                       use_scheduler=None, num_epochs=10, batch_size=64, \n",
    "                       model_saving_folder=None, model_name=None, stop_signal_file=None):\n",
    "    print(\"'train_and_validate' function started. \\n\")\n",
    "    # Ensure model saving folder exists (deleting existing first if there is one)\n",
    "    if model_saving_folder and os.path.exists(model_saving_folder):\n",
    "        # os.rmdir(model_saving_folder) # Only works on empty folders \n",
    "        shutil.rmtree(model_saving_folder) # Safely remove all contents\n",
    "        if not os.path.exists(model_saving_folder):\n",
    "            print(f\"Existing folder has been removed : {model_saving_folder}\\n\")\n",
    "    if model_saving_folder and not os.path.exists(model_saving_folder):\n",
    "        os.makedirs(model_saving_folder)\n",
    "        \n",
    "    if not model_saving_folder:\n",
    "        model_saving_folder = './saved_models'\n",
    "    if not model_name:\n",
    "        model_name = 'model'\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert data to tensors # Returns a copy, original is safe\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)  # (seqs, seq_len, features)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)    # (seqs, seq_len)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"y_train:\")\n",
    "    print(type(y_train))\n",
    "    print(y_train.dtype)\n",
    "    print(y_train.shape)\n",
    "    print(\"X_train:\")\n",
    "    print(type(X_train))\n",
    "    print(X_train.dtype)\n",
    "    print(X_train.shape)\n",
    "    print(\"\\ny_val:\")\n",
    "    print(type(y_val))\n",
    "    print(y_val.dtype)\n",
    "    print(y_val.shape)\n",
    "    print(\"X_val:\")\n",
    "    print(type(X_val))\n",
    "    print(X_val.dtype)\n",
    "    print(X_val.shape)\n",
    "\n",
    "    # Debug prints for TensorDataset and DataLoader\n",
    "    print(\"\\nDataset Lengths:\")\n",
    "    print(f\"Train Dataset Length: {len(train_dataset)}\")\n",
    "    print(f\"Validation Dataset Length: {len(val_dataset)}\")\n",
    "\n",
    "    print(\"\\nDataLoader Batch Sizes:\")\n",
    "    print(f\"Number of Batches in Train DataLoader: {len(train_loader)}\")\n",
    "    print(f\"Number of Batches in Validation DataLoader: {len(val_loader)}\")\n",
    "\n",
    "    # Additional details for y_train, y_val, and y_test\n",
    "    print(\"\\ny_train Unique Values and Stats:\")\n",
    "    print(f\"Unique values in y_train: {y_train.unique()}\")\n",
    "    print(f\"y_train Min: {y_train.min()}, Max: {y_train.max()}\")\n",
    "\n",
    "    print(\"\\ny_val Unique Values and Stats:\")\n",
    "    print(f\"Unique values in y_val: {y_val.unique()}\")\n",
    "    print(f\"y_val Min: {y_val.min()}, Max: {y_val.max()}\")\n",
    "\n",
    "    # Device check\n",
    "    print(\"\\nDevice Info:\")\n",
    "    print(f\"X_train Device: {X_train.device}\")\n",
    "    print(f\"y_train Device: {y_train.device}\")\n",
    "    print(f\"X_val Device: {X_val.device}\")\n",
    "    print(f\"y_val Device: {y_val.device}\\n\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    # num_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "    global best_results  # Ensure we can modify the external variable if defined outside.\n",
    "    best_results = []    # Start empty each training run\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        class_correct = {}  # Dictionary to store correct predictions per class\n",
    "        class_total = {}  # Dictionary to store total samples per class\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nStop signal detected. Exiting training loop safely.\\n\")\n",
    "            break\n",
    "        model.train()\n",
    "        i=0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Reset gradients before forward pass\n",
    "            optimizer.zero_grad()  # Best practice\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            outputs = outputs.view(-1, output_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            if epoch == 1 and i < 3:\n",
    "                i += 1\n",
    "                print(f\"\\nUnique target values: {y_batch.unique()}\")\n",
    "                print(f\"Target dtype: {y_batch.dtype}\")\n",
    "                print(f\"Min target: {y_batch.min()}, Max target: {y_batch.max()}\")\n",
    "                print(\"Unique classes in y_train:\", y_train.unique())\n",
    "                print(f\"Unique classes in y_val: {y_val.unique()}\\n\")\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Compute class-wise accuracy (Accumulates values in dict)\n",
    "            compute_classwise_accuracy(outputs, y_batch, class_correct, class_total)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            # No longer reset gradients here: optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * X_batch.size(0)  # Scale back to total loss\n",
    "            \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Compute per-class training accuracy\n",
    "        # train_classwise_accuracy = {int(c): (class_correct[c] / class_total[c]) * 100 if class_total[c] > 0 else 0 \n",
    "        #                            for c in sorted(class_total.keys())}\n",
    "        train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "                                    for c in sorted(class_total.keys())}\n",
    "        \n",
    "        # Perform validation at the end of each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_class_correct = {}\n",
    "        val_class_total = {}\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = model(X_val_batch).view(-1, output_size)\n",
    "                val_labels = y_val_batch.view(-1)\n",
    "                val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)  # Scale to total loss\n",
    "                val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "                val_correct += (val_predictions == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "                # Compute per-class validation accuracy\n",
    "                compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Compute per-class validation accuracy\n",
    "        # val_classwise_accuracy = {int(c): (val_class_correct[c] / val_class_total[c]) * 100 if val_class_total[c] > 0 else 0 \n",
    "        #                          for c in sorted(val_class_total.keys())}\n",
    "        val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "                                  for c in sorted(val_class_total.keys())}\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "              f\"Val Loss: {val_loss:.9f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "\n",
    "        # Save current model and update best results if applicable\n",
    "        current_epoch_info = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'], # Optimizer state\n",
    "            \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        }\n",
    "\n",
    "        # Insert this epoch if we have fewer than 5 results\n",
    "        # or if it beats the lowest of the top 5\n",
    "        if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "            if len(best_results) == 5:\n",
    "                # Remove the worst model from the list, the last (lowest accuracy)\n",
    "                worst = best_results.pop() \n",
    "                if os.path.exists(worst[\"model_path\"]):\n",
    "                    os.remove(worst[\"model_path\"])\n",
    "                    print(f\"Removed old model with accuracy {worst['val_accuracy']*100:.2f}%, and file was at {worst['model_path']}\")\n",
    "            # Just insert and sort by val_accuracy descending\n",
    "            best_results.append(current_epoch_info) \n",
    "            best_results.sort(key=lambda x: x[\"val_accuracy\"], reverse=True)\n",
    "            torch.save({ # Save this model\n",
    "                'epoch': epoch+1,  # Save the current epoch\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'model_state_dict': model.state_dict(),  # Model weights\n",
    "                'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "                'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "            }, current_epoch_info[\"model_path\"])\n",
    "            print(f\"Model saved after epoch {epoch+1} to {current_epoch_info['model_path']} \\n\")\n",
    "\n",
    "        if use_scheduler == True:\n",
    "            # Scheduler step should follow after considering the results (placed after otallher losses)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    # Save the final model\n",
    "    if current_epoch_info:\n",
    "        final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "        torch.save({ # Save this model\n",
    "            'epoch': epoch+1,  # Save the current epoch\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'model_state_dict': model.state_dict(),  # Model weights\n",
    "            'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "        }, final_model_path)\n",
    "        print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "\n",
    "    print(\"\\nTraining complete. \\n\\nTop 5 Models Sorted by Validation Accuracy: \")\n",
    "    for res in best_results:        \n",
    "        print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "              f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \" \n",
    "              f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "              f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    del X_train, y_train, X_val, y_val, train_dataset, val_dataset, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the checkpoint\n",
    "    # checkpoint = torch.load(\"path/to/model_checkpoint.pth\")\n",
    "    # # Restore model state\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # # Restore optimizer state\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # # Restore scheduler state (if used)\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    # # Restore epoch and other metadata\n",
    "    # start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "    # loss = checkpoint['loss']  # Optional\n",
    "    # print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "def compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total):\n",
    "    \"\"\"\n",
    "    Computes per-class accuracy by accumulating correct and total samples for each class.\n",
    "    \n",
    "    student_logits_flat: Model predictions (logits) in shape [batch_size * seq_len, output_size]\n",
    "    y_batch: True labels in shape [batch_size * seq_len]\n",
    "    class_correct: Dictionary to store correct predictions per class\n",
    "    class_total: Dictionary to store total samples per class\n",
    "    \"\"\"\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = torch.argmax(student_logits_flat, dim=-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "    # Loop through batch elements to track correct/total per class\n",
    "    for label, pred in zip(y_batch.cpu().numpy(), predictions.cpu().numpy()):\n",
    "        if label not in class_total:\n",
    "            class_total[label] = 0\n",
    "            class_correct[label] = 0\n",
    "        class_total[label] += 1\n",
    "        if label == pred:\n",
    "            class_correct[label] += 1\n",
    "\n",
    "\n",
    "# Training and validation function for Period 2 and beyond \n",
    "def train_and_validate_lora(student_model, teacher_model, stable_classes, output_size, criterion, optimizer, \n",
    "                            X_train, y_train, X_val, y_val, scheduler, \n",
    "                            use_scheduler=None, num_epochs=10, batch_size=64, alpha=0.5,\n",
    "                            model_saving_folder=None, model_name=None, stop_signal_file=None):\n",
    "    \"\"\"\n",
    "    student_model: The new LoRA-based student model (with output size 3).\n",
    "    teacher_model: Frozen teacher model from period 1 (with output size 2).\n",
    "    criterion: CrossEntropyLoss function.\n",
    "    optimizer: Optimizer for student model.\n",
    "    X_train, y_train, X_val, y_val: Training/validation data (as NumPy arrays or similar).\n",
    "    num_epochs: Number of epochs to train.\n",
    "    batch_size: Batch size for DataLoader.\n",
    "    alpha: Weighting factor for distillation loss (alpha * distill_loss + (1-alpha) * ce_loss).\n",
    "    \"\"\"\n",
    "    print(\"'train_and_validate' function started. \\n\")\n",
    "    # Ensure model saving folder exists (deleting existing first if there is one)\n",
    "    if model_saving_folder and os.path.exists(model_saving_folder):\n",
    "        # os.rmdir(model_saving_folder) # Only works on empty folders \n",
    "        shutil.rmtree(model_saving_folder) # Safely remove all contents\n",
    "        if not os.path.exists(model_saving_folder):\n",
    "            print(f\"Existing folder has been removed : {model_saving_folder}\\n\")\n",
    "    if model_saving_folder and not os.path.exists(model_saving_folder):\n",
    "        os.makedirs(model_saving_folder)\n",
    "        \n",
    "    if not model_saving_folder:\n",
    "        model_saving_folder = './saved_models'\n",
    "    if not model_name:\n",
    "        model_name = 'model'\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "\n",
    "    # Convert data to tensors # Returns a copy, original is safe\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)  # (seqs, seq_len, features)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)    # (seqs, seq_len)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"y_train:\")\n",
    "    print(type(y_train))\n",
    "    print(y_train.dtype)\n",
    "    print(y_train.shape)\n",
    "    print(\"X_train:\")\n",
    "    print(type(X_train))\n",
    "    print(X_train.dtype)\n",
    "    print(X_train.shape)\n",
    "    print(\"\\ny_val:\")\n",
    "    print(type(y_val))\n",
    "    print(y_val.dtype)\n",
    "    print(y_val.shape)\n",
    "    print(\"X_val:\")\n",
    "    print(type(X_val))\n",
    "    print(X_val.dtype)\n",
    "    print(X_val.shape)\n",
    "\n",
    "    # Debug prints for TensorDataset and DataLoader\n",
    "    print(\"\\nDataset Lengths:\")\n",
    "    print(f\"Train Dataset Length: {len(train_dataset)}\")\n",
    "    print(f\"Validation Dataset Length: {len(val_dataset)}\")\n",
    "\n",
    "    print(\"\\nDataLoader Batch Sizes:\")\n",
    "    print(f\"Number of Batches in Train DataLoader: {len(train_loader)}\")\n",
    "    print(f\"Number of Batches in Validation DataLoader: {len(val_loader)}\")\n",
    "\n",
    "    # Additional details for y_train, y_val, and y_test\n",
    "    print(\"\\ny_train Unique Values and Stats:\")\n",
    "    print(f\"Unique values in y_train: {y_train.unique()}\")\n",
    "    print(f\"y_train Min: {y_train.min()}, Max: {y_train.max()}\")\n",
    "\n",
    "    print(\"\\ny_val Unique Values and Stats:\")\n",
    "    print(f\"Unique values in y_val: {y_val.unique()}\")\n",
    "    print(f\"y_val Min: {y_val.min()}, Max: {y_val.max()}\")\n",
    "\n",
    "    # Device check\n",
    "    print(\"\\nDevice Info:\")\n",
    "    print(f\"X_train Device: {X_train.device}\")\n",
    "    print(f\"y_train Device: {y_train.device}\")\n",
    "    print(f\"X_val Device: {X_val.device}\")\n",
    "    print(f\"y_val Device: {y_val.device}\\n\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    # num_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "    global best_results  # Ensure we can modify the external variable if defined outside.\n",
    "    best_results = []    # Start empty each training run\n",
    "\n",
    "    teacher_model.eval()  # Ensure teacher is frozen\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        class_correct = {}  # Dictionary to store correct predictions per class\n",
    "        class_total = {}  # Dictionary to store total samples per class\n",
    "        if stop_signal_file and os.path.exists(stop_signal_file):\n",
    "            print(\"\\nStop signal detected. Exiting training loop safely.\\n\")\n",
    "            break\n",
    "        student_model.train()\n",
    "        i=0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Reset gradients before forward pass\n",
    "            optimizer.zero_grad()  # Best practice\n",
    "\n",
    "            # Forward pass: student model produces logits for output_size classes.\n",
    "            student_logits = student_model(X_batch)  # Shape: [batch, seq_len, output_size]\n",
    "            \n",
    "            # Reshape for CE loss computation.\n",
    "            student_logits_flat = student_logits.view(-1, output_size)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            # Compute Cross-Entropy loss\n",
    "            ce_loss = criterion(student_logits_flat, y_batch)\n",
    "\n",
    "            # Compute class-wise accuracy (Accumulates values in dict)\n",
    "            compute_classwise_accuracy(student_logits_flat, y_batch, class_correct, class_total)\n",
    "\n",
    "            if epoch == 1 and i < 3:\n",
    "                i += 1\n",
    "                print(f\"\\nUnique target values: {y_batch.unique()}\")\n",
    "                print(f\"Target dtype: {y_batch.dtype}\")\n",
    "                print(f\"Min target: {y_batch.min()}, Max target: {y_batch.max()}\")\n",
    "                print(\"Unique classes in y_train:\", y_train.unique())\n",
    "                print(f\"Unique classes in y_val: {y_val.unique()}\\n\")\n",
    "\n",
    "            # Knowledge Distillation:\n",
    "            # Forward pass through teacher (pre-trained on previous period data).\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(X_batch)  # Shape: [batch, seq_len, teacher_output_size]\n",
    "            # Use stable_classes (a list of indices) to extract the relevant logits.\n",
    "            # We distill only the stable classes (class 1 if teacher is from period 1).\n",
    "            # Teacher's class index 1 corresponds to student's class index 1.\n",
    "            # It's safer to use index_select to ensure the operation works on GPU.\n",
    "            stable_indices = torch.tensor(stable_classes, device=teacher_logits.device)\n",
    "            teacher_stable = teacher_logits.index_select(dim=2, index=stable_indices)\n",
    "            student_stable = student_logits.index_select(dim=2, index=stable_indices)\n",
    "\n",
    "            # Compute the distillation loss over the selected stable classes.\n",
    "            distill_loss = F.mse_loss(student_stable, teacher_stable)\n",
    "            \n",
    "            # Total loss: balance between cross-entropy and distillation loss.\n",
    "            total_loss = alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item() * X_batch.size(0)\n",
    "            \n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Compute per-class training accuracy\n",
    "        # train_classwise_accuracy = {int(c): (class_correct[c] / class_total[c]) * 100 if class_total[c] > 0 else 0 \n",
    "        #                            for c in sorted(class_total.keys())}\n",
    "        train_classwise_accuracy = {int(c): f\"{(class_correct[c] / class_total[c]) * 100:.2f}%\" if class_total[c] > 0 else \"0.00%\" \n",
    "                                    for c in sorted(class_total.keys())}\n",
    "\n",
    "        # Perform validation at the end of each epoch (only CE loss and accuracy)\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_class_correct = {}\n",
    "        val_class_total = {}\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                val_outputs = student_model(X_val_batch).view(-1, output_size)\n",
    "                val_labels = y_val_batch.view(-1)\n",
    "                val_loss += criterion(val_outputs, val_labels).item() * X_val_batch.size(0)  # Scale to total loss\n",
    "                val_predictions = torch.argmax(val_outputs, dim=-1)\n",
    "                val_correct += (val_predictions == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "                # Compute per-class validation accuracy\n",
    "                compute_classwise_accuracy(val_outputs, val_labels, val_class_correct, val_class_total)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Compute per-class validation accuracy\n",
    "        # val_classwise_accuracy = {int(c): (val_class_correct[c] / val_class_total[c]) * 100 if val_class_total[c] > 0 else 0 \n",
    "        #                          for c in sorted(val_class_total.keys())}\n",
    "        val_classwise_accuracy = {int(c): f\"{(val_class_correct[c] / val_class_total[c]) * 100:.2f}%\" if val_class_total[c] > 0 else \"0.00%\" \n",
    "                                  for c in sorted(val_class_total.keys())}\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \"\n",
    "              f\"Val Loss: {val_loss:.9f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.9f}\")\n",
    "\n",
    "        # Save current model and update best results if applicable\n",
    "        current_epoch_info = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_classwise_accuracy\": train_classwise_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_classwise_accuracy\": val_classwise_accuracy,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'], # Optimizer state\n",
    "            \"model_path\": os.path.join(model_saving_folder, f\"{model_name}_epoch_{epoch+1}.pth\")\n",
    "        }\n",
    "\n",
    "        # Insert this epoch if we have fewer than 5 results\n",
    "        # or if it beats the lowest of the top 5\n",
    "        if len(best_results) < 5 or val_accuracy > best_results[-1][\"val_accuracy\"]:\n",
    "            if len(best_results) == 5:\n",
    "                # Remove the worst model from the list, the last (lowest accuracy)\n",
    "                worst = best_results.pop() \n",
    "                if os.path.exists(worst[\"model_path\"]):\n",
    "                    os.remove(worst[\"model_path\"])\n",
    "                    print(f\"Removed old model with accuracy {worst['val_accuracy']*100:.2f}%, and file was at {worst['model_path']}\")\n",
    "            # Just insert and sort by val_accuracy descending\n",
    "            best_results.append(current_epoch_info) \n",
    "            best_results.sort(key=lambda x: x[\"val_accuracy\"], reverse=True)\n",
    "            torch.save({ # Save this model\n",
    "                'epoch': epoch+1,  # Save the current epoch\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'model_state_dict': student_model.state_dict(),  # Model weights\n",
    "                'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "                'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "            }, current_epoch_info[\"model_path\"])\n",
    "            print(f\"Model saved after epoch {epoch+1} to {current_epoch_info['model_path']} \\n\")\n",
    "\n",
    "        if use_scheduler == True:\n",
    "            # Scheduler step should follow after considering the results (placed after otallher losses)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    # Save the final model\n",
    "    if current_epoch_info:\n",
    "        final_model_path = os.path.join(model_saving_folder, f\"{model_name}_final.pth\")\n",
    "        torch.save({ # Save this model\n",
    "            'epoch': epoch+1,  # Save the current epoch\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'model_state_dict': student_model.state_dict(),  # Model weights\n",
    "            'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "            'learning_rate': optimizer.param_groups[0]['lr'] # Optimizer state\n",
    "        }, final_model_path)\n",
    "        print(f\"\\nFinal model saved to {final_model_path}\")\n",
    "\n",
    "    print(\"\\nTraining complete. \\n\\nTop 5 Models Sorted by Validation Accuracy: \")\n",
    "    for res in best_results:        \n",
    "        print(f\"Epoch {res['epoch']}/{num_epochs}, \"\n",
    "              f\"Train Loss: {res['train_loss']:.9f}, \"\n",
    "              f\"Train-Class-Acc: {train_classwise_accuracy}, \" \n",
    "              f\"Val Loss: {res['val_loss']:.9f}, \"\n",
    "              f\"Val Accuracy: {res['val_accuracy'] * 100:.2f}%, \"\n",
    "              f\"Val-Class-Acc: {val_classwise_accuracy}, \"\n",
    "              f\"Model Path: {res['model_path']}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    del X_train, y_train, X_val, y_val, train_dataset, val_dataset, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch compiled CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch compiled CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.4.1+cu124\n",
      "CUDA Available: True\n",
      "Num GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4070\n",
      "Seeding successful!\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Num GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(\"Seeding successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_period_files_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data\\\\Polygon_BTCUSD_4Y_1min\\\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'BTCUSD'\n",
    "file_name = f'Polygon_{pair}_4Y_1min'  # File name for saving data\n",
    "# BASE_FOLDER_PATH = f\"{Working_directory}/Data/{file_name}\"\n",
    "BASE_FOLDER_PATH = f\"Data/{file_name}\"\n",
    "# folder_path=f'{BASE_FOLDER_PATH}/polygon_io/12_USD_Crypto_Pairs/{file_name}'\n",
    "folder_path=f'{BASE_FOLDER_PATH}'\n",
    "if not os.path.isdir(folder_path):\n",
    "    raise FileNotFoundError(f\"Directory '{folder_path}' does not exist.\")\n",
    "file_path=f'{folder_path}/{file_name}.csv'\n",
    "number_days = 190\n",
    "with_indicators_file_path = os.path.normpath(f'{folder_path}/_{file_name}_{number_days}_days_with_indicators.csv')\n",
    "\n",
    "# LABORATORY\\_Global_Pytorch\\Continual_Learning\\Data\\Polygon_BTCUSD_4Y_1min\n",
    "\n",
    "list_period_files_full_path = [\n",
    "    # Period 1\n",
    "    with_indicators_file_path,\n",
    "    # Period 2\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\"),\n",
    "    # Period 3\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\"),\n",
    "    # Period 4\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\"),\n",
    "    # Period 5\n",
    "    os.path.normpath(f\"{folder_path}/Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\")\n",
    "]\n",
    "\n",
    "with_indicators_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: Polygon_BTCUSD_4Y_1min.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2020-11-11__2021-05-20__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-05-20__2021-11-26__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2021-11-26__2022-06-04__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11.png\n",
      "Found file: Polygon_BTCUSD_4Y_1min_190_days__2022-06-04__2022-12-11__with_indicators.csv\n",
      "Found file: Polygon_BTCUSD_4Y_1min_525_days.png\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days.png\n",
      "Found file: _Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(folder_path):\n",
    "    print(f\"Found file: {file}\")\n",
    "\n",
    "# Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File path: Data\\Polygon_BTCUSD_4Y_1min\\_Polygon_BTCUSD_4Y_1min_190_days_with_indicators.csv\n",
      "\n",
      "data_retrieved - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "data_gaussian - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "columns_to_transform = \n",
      "Index(['open', 'high', 'low', 'close', 'SMA_5', 'SMA_10', 'EMA_10'], dtype='object'), \n",
      "len(columns_to_transform) = 7\n",
      "\n",
      "data_log_return - Missing timestamps time: \n",
      "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='min')\n",
      "\n",
      "No missing timestamps.\n",
      "\n",
      "    Number of sequences:\n",
      "        - sequences[0].shape: (1000, 7)\n",
      "        - Total sequences: 4543\n",
      "        - Train sequences: 3634\n",
      "        - Validation sequences: 454\n",
      "        - Test sequences: 455\n",
      "    \n",
      "close_col_index = 3\n",
      "Number_features = 7\n",
      "unique_classes = [0 1 2 3 4]\n",
      "num_classes = 5\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "    with_indicators_file_path = list_period_files_full_path[0],\n",
    "    downsampled_data_minutes = downsampled_data_minutes,\n",
    "    exclude_columns = exclude_columns,\n",
    "    lower_threshold = lower_threshold,\n",
    "    upper_threshold = upper_threshold,\n",
    "    reverse_steps = reverse_steps,\n",
    "    sequence_length = sequence_length,\n",
    "    sliding_interval = sliding_interval,\n",
    "    trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    ")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n",
    "\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test, Number_features\n",
    "del unique_classes, num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __*All periods data*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n- 'trend': Categorized trend values based on the detected phases:\\n    - 0: No trend\\n    - 1: Moderate negative trend\\n    - 2: Very strong negative trend\\n    - 3: Moderate positive trend\\n    - 4: Very strong positive trend\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 'trend': Categorized trend values based on the detected phases:\n",
    "    - 0: No trend\n",
    "    - 1: Moderate negative trend\n",
    "    - 2: Very strong negative trend\n",
    "    - 3: Moderate positive trend\n",
    "    - 4: Very strong positive trend\n",
    "\"\"\"\n",
    "\n",
    "# Initialize empty lists to combine validation elements\n",
    "# X_train_all = []\n",
    "# y_train_all = []\n",
    "# X_val_all = []\n",
    "# y_val_all = []\n",
    "# X_test_all = []\n",
    "# y_test_all = []\n",
    "\n",
    "# for path_ in list_period_files_full_path:\n",
    "#     with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#         X_train_, y_train_, X_val_, y_val_, X_test_, y_test_, Number_features = process_and_return_splits(\n",
    "#             with_indicators_file_path = path_,\n",
    "#             downsampled_data_minutes = downsampled_data_minutes,\n",
    "#             exclude_columns = exclude_columns,\n",
    "#             lower_threshold = lower_threshold,\n",
    "#             upper_threshold = upper_threshold,\n",
    "#             reverse_steps = reverse_steps,\n",
    "#             sequence_length = sequence_length,\n",
    "#             sliding_interval = sliding_interval,\n",
    "#             trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "#         )\n",
    "#     # Combine validation elements\n",
    "#     X_train_all.extend(X_train_)\n",
    "#     y_train_all.extend(y_train_)\n",
    "#     X_val_all.extend(X_val_)\n",
    "#     y_val_all.extend(y_val_)\n",
    "#     X_test_all.extend(X_test_)\n",
    "#     y_test_all.extend(y_test_)\n",
    "#     # Delete unused variables to save memory\n",
    "#     del X_train_, y_train_, X_val_, y_val_, X_test_, y_test_\n",
    "\n",
    "# X_train_all = np.array(X_train_all)\n",
    "# y_train_all = np.array(y_train_all)\n",
    "# X_val_all = np.array(X_val_all)\n",
    "# y_val_all = np.array(y_val_all)\n",
    "# X_test_all = np.array(X_test_all)\n",
    "# y_test_all = np.array(y_test_all)\n",
    "\n",
    "\n",
    "# with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "#         with_indicators_file_path = list_period_files_full_path[0],\n",
    "#         downsampled_data_minutes = downsampled_data_minutes,\n",
    "#         exclude_columns = exclude_columns,\n",
    "#         lower_threshold = lower_threshold,\n",
    "#         upper_threshold = upper_threshold,\n",
    "#         reverse_steps = reverse_steps,\n",
    "#         sequence_length = sequence_length,\n",
    "#         sliding_interval = sliding_interval,\n",
    "#         trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "#         # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "#     )\n",
    "    \n",
    "# print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "# unique_classes = np.unique(y_val)\n",
    "# num_classes = len(unique_classes)\n",
    "# print(f\"unique_classes = {unique_classes}\")\n",
    "# print(f\"num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period 1 --> Training and saving in __*'1st_try'*__ (BiGRUWithAttention, num_layers = 4) ---> Val acc = 98.35 %\n",
    "### Val-Class-Acc: {0: '98.63%', 1: '97.92%'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number_features = 7\n",
      "unique_classes = [0 1]\n",
      "num_classes = 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- 'trend': Categorized trend values based on the detected phases:\n",
    "    - 0: No trend\n",
    "    - 1: Moderate negative trend\n",
    "    - 2: Very strong negative trend\n",
    "    - 3: Moderate positive trend\n",
    "    - 4: Very strong positive trend\n",
    "\"\"\"\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, Number_features = process_and_return_splits(\n",
    "        with_indicators_file_path = list_period_files_full_path[0], # Change \n",
    "        downsampled_data_minutes = downsampled_data_minutes,\n",
    "        exclude_columns = exclude_columns,\n",
    "        lower_threshold = lower_threshold,\n",
    "        upper_threshold = upper_threshold,\n",
    "        reverse_steps = reverse_steps,\n",
    "        sequence_length = sequence_length,\n",
    "        sliding_interval = sliding_interval,\n",
    "        trends_to_keep = {0, 1}  # Default keeps all trends : {0, 1, 2, 3, 4}\n",
    "        # trends_to_keep = {0, 1, 2, 3, 4}  # Default keeps all trends\n",
    "    )\n",
    "\n",
    "print(f\"\\nNumber_features = {Number_features}\")\n",
    "\n",
    "unique_classes = np.unique(y_val)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"unique_classes = {unique_classes}\")\n",
    "print(f\"num_classes = {num_classes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medical_AI_LoRA_py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
